{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13f4deb",
   "metadata": {},
   "source": [
    "# Comparing LLM Models with Cellmage\n",
    "\n",
    "This notebook compares the performance of different LLM models using the cellmage library.\n",
    "We'll be comparing:\n",
    "- OpenAI GPT-4.1-nano\n",
    "- Google Gemini-2.5-flash\n",
    "\n",
    "**Date:** April 24, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307baf0",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Let's set up our development environment and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f2b8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:02:48,516 - cellmage - INFO - Cellmage logging initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /Users/tpinto/madpin/cellmage/notebooks\n",
      "Project root directory: /Users/tpinto/madpin/cellmage\n",
      "Added path: /Users/tpinto/madpin/cellmage\n",
      "Cellmage version: 0.1.0\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Skip dotenv loading for testing\n",
    "os.environ[\"CELLMAGE_SKIP_DOTENV\"] = \"1\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Ensure the cellmage package can be imported\n",
    "# Get the absolute path of the current working directory\n",
    "notebook_dir = os.getcwd()\n",
    "# Get the project root directory (parent of the notebook directory)\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "\n",
    "print(f\"Notebook directory: {notebook_dir}\")\n",
    "print(f\"Project root directory: {project_root}\")\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added path: {project_root}\")\n",
    "\n",
    "try:\n",
    "    # Import cellmage\n",
    "    import cellmage\n",
    "\n",
    "    # Check version - handle case where __version__ might not be available\n",
    "    try:\n",
    "        print(f\"Cellmage version: {cellmage.__version__}\")\n",
    "    except AttributeError:\n",
    "        print(\"Cellmage imported successfully, but version information is not available\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Error importing cellmage: {e}\")\n",
    "    print(\"\\nDebug information:\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Python path: {sys.path}\")\n",
    "    print(\"\\nTry running this notebook from the project root directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad3444",
   "metadata": {},
   "source": [
    "## Helper Functions for Model Comparison\n",
    "\n",
    "Let's define some helper functions to assist with our model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6737dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_manager(model_name):\n",
    "    \"\"\"Create a chat manager for the specified model.\"\"\"\n",
    "    from cellmage.adapters.direct_client import DirectLLMAdapter\n",
    "    from cellmage.resources.memory_loader import MemoryLoader\n",
    "    from cellmage.storage.memory_store import MemoryStore\n",
    "\n",
    "    # Create LLM client\n",
    "    llm_client = DirectLLMAdapter()\n",
    "    # Set the model name as an override\n",
    "    llm_client.set_override(\"model\", model_name)\n",
    "\n",
    "    # Create components\n",
    "    persona_loader = MemoryLoader()\n",
    "    snippet_provider = MemoryLoader()\n",
    "    history_store = MemoryStore()\n",
    "\n",
    "    # Add standard persona\n",
    "    persona_loader.add_persona(\n",
    "        name=\"standard_persona\",\n",
    "        system_message=\"You are a helpful assistant that provides accurate and concise information.\",\n",
    "        config={\"temperature\": 0.5},\n",
    "    )\n",
    "\n",
    "    # Create chat manager\n",
    "    chat_manager = cellmage.ChatManager(\n",
    "        llm_client=llm_client,\n",
    "        persona_loader=persona_loader,\n",
    "        snippet_provider=snippet_provider,\n",
    "        history_store=history_store,\n",
    "    )\n",
    "\n",
    "    chat_manager.set_default_persona(\"standard_persona\")\n",
    "    return chat_manager\n",
    "\n",
    "\n",
    "def time_response(chat_manager, prompt):\n",
    "    \"\"\"Time how long it takes to get a response.\"\"\"\n",
    "    # Clear history to ensure fresh context\n",
    "    chat_manager.clear_history()\n",
    "\n",
    "    # Time the response\n",
    "    start_time = time.time()\n",
    "    response = chat_manager.chat(prompt, stream=False)\n",
    "    end_time = time.time()\n",
    "\n",
    "    duration = end_time - start_time\n",
    "    response_length = len(response) if response else 0\n",
    "\n",
    "    return {\n",
    "        \"duration\": duration,\n",
    "        \"response\": response,\n",
    "        \"length\": response_length,\n",
    "        \"tokens_per_second\": response_length / (4 * duration) if duration > 0 else 0,  # Rough estimate\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_models(prompts, models=[\"gpt-4.1-nano\", \"gemini-2.5-flash\"]):\n",
    "    \"\"\"Compare multiple models across multiple prompts.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"\\n===== Setting up {model} =====\")\n",
    "        chat_manager = create_chat_manager(model)\n",
    "        results[model] = []\n",
    "\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(f\"\\n- Testing prompt {i + 1}/{len(prompts)} on {model}\")\n",
    "            try:\n",
    "                result = time_response(chat_manager, prompt)\n",
    "                print(f\"  Duration: {result['duration']:.2f}s, Length: {result['length']} chars\")\n",
    "                results[model].append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error with {model}: {str(e)}\")\n",
    "                results[model].append({\"error\": str(e)})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84684a",
   "metadata": {},
   "source": [
    "## Test Prompts\n",
    "\n",
    "Let's define a variety of test prompts to evaluate different aspects of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ec209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    # 1. Simple factual question\n",
    "    \"What is the capital of France and what are three interesting facts about it?\",\n",
    "    # 2. Code generation task\n",
    "    \"\"\"Write a Python function to check if a string is a palindrome. \n",
    "    The function should ignore spaces, punctuation, and capitalization.\"\"\",\n",
    "    # 3. Creative writing task\n",
    "    \"\"\"Write a short story (around 150 words) about a robot that develops \n",
    "    emotions and how it navigates its first human friendship.\"\"\",\n",
    "    # 4. Analytical reasoning task\n",
    "    \"\"\"What are the ethical considerations when implementing AI in healthcare? \n",
    "    Consider at least three perspectives and discuss potential solutions to ethical dilemmas.\"\"\",\n",
    "    # 5. Technical explanation task\n",
    "    \"\"\"Explain how public key cryptography works in simple terms that a high school \n",
    "    student could understand. Include an analogy to help explain it.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba970128",
   "metadata": {},
   "source": [
    "## Running the Comparison\n",
    "\n",
    "Now let's run our comparison between GPT-4.1-nano and Gemini-2.5-flash models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99524a89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the comparison (Note: This may take some time to complete)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting model comparison at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m comparison_results \u001b[38;5;241m=\u001b[39m compare_models(test_prompts)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mComparison completed at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the comparison (Note: This may take some time to complete)\n",
    "print(f\"Starting model comparison at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "comparison_results = compare_models(test_prompts)\n",
    "print(f\"\\nComparison completed at {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b5a69",
   "metadata": {},
   "source": [
    "## Analyzing the Results\n",
    "\n",
    "Let's analyze the results of our model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a63216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a summary dataframe\n",
    "summary_data = []\n",
    "\n",
    "for model, results in comparison_results.items():\n",
    "    for i, result in enumerate(results):\n",
    "        if \"error\" not in result:\n",
    "            summary_data.append(\n",
    "                {\n",
    "                    \"Model\": model,\n",
    "                    \"Prompt\": f\"Prompt {i + 1}\",\n",
    "                    \"Time (s)\": result[\"duration\"],\n",
    "                    \"Response Length\": result[\"length\"],\n",
    "                    \"Est. Tokens/Second\": result[\"tokens_per_second\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Compare average response time and length\n",
    "avg_stats = (\n",
    "    summary_df.groupby(\"Model\")\n",
    "    .agg({\"Time (s)\": \"mean\", \"Response Length\": \"mean\", \"Est. Tokens/Second\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\nAverage Statistics by Model:\")\n",
    "print(avg_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da9b90d",
   "metadata": {},
   "source": [
    "## Visualizing the Comparison\n",
    "\n",
    "Let's create some visualizations to better understand the differences between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01030c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Response Times by Prompt\n",
    "plt.subplot(2, 2, 1)\n",
    "for model in summary_df[\"Model\"].unique():\n",
    "    model_data = summary_df[summary_df[\"Model\"] == model]\n",
    "    plt.plot(model_data[\"Prompt\"], model_data[\"Time (s)\"], marker=\"o\", label=model)\n",
    "plt.title(\"Response Time by Prompt\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Response Length by Prompt\n",
    "plt.subplot(2, 2, 2)\n",
    "for model in summary_df[\"Model\"].unique():\n",
    "    model_data = summary_df[summary_df[\"Model\"] == model]\n",
    "    plt.plot(model_data[\"Prompt\"], model_data[\"Response Length\"], marker=\"o\", label=model)\n",
    "plt.title(\"Response Length by Prompt\")\n",
    "plt.ylabel(\"Length (characters)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Average Response Time\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(avg_stats[\"Model\"], avg_stats[\"Time (s)\"])\n",
    "plt.title(\"Average Response Time\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Plot 4: Estimated Tokens per Second\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.bar(avg_stats[\"Model\"], avg_stats[\"Est. Tokens/Second\"])\n",
    "plt.title(\"Estimated Tokens per Second\")\n",
    "plt.ylabel(\"Tokens/Second\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec373dc",
   "metadata": {},
   "source": [
    "## Qualitative Analysis\n",
    "\n",
    "Now let's look at some selected responses for qualitative comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d468129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a couple of interesting prompts for detailed comparison\n",
    "interesting_prompts = [0, 2]  # Factual and Creative tasks\n",
    "\n",
    "for i in interesting_prompts:\n",
    "    print(f\"\\n===== Prompt {i + 1}: {test_prompts[i][:50]}... =====\")\n",
    "\n",
    "    for model, results in comparison_results.items():\n",
    "        if i < len(results) and \"error\" not in results[i]:\n",
    "            print(f\"\\n{model} response (Time: {results[i]['duration']:.2f}s):\")\n",
    "            print(\"-\" * 50)\n",
    "            print(results[i][\"response\"][:500] + (\"...\" if len(results[i][\"response\"]) > 500 else \"\"))\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ca21a",
   "metadata": {},
   "source": [
    "## Testing with a Complex Coding Task\n",
    "\n",
    "Let's test both models with a more complex coding task to evaluate their programming capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06998715",
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_coding_prompt = \"\"\"\n",
    "Create a Python class for a simple task management system with these features:\n",
    "\n",
    "1. Add tasks with priority levels (high, medium, low)\n",
    "2. Mark tasks as complete\n",
    "3. List tasks filtered by priority or completion status\n",
    "4. Delete tasks\n",
    "5. Export tasks to JSON format\n",
    "\n",
    "Include appropriate error handling, documentation, and an example usage of the class.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing complex coding task...\")\n",
    "coding_results = {}\n",
    "\n",
    "for model in [\"gpt-4.1-nano\", \"gemini-2.5-flash\"]:\n",
    "    print(f\"\\nTesting {model}...\")\n",
    "\n",
    "    try:\n",
    "        chat_manager = create_chat_manager(model)\n",
    "        start_time = time.time()\n",
    "        response = chat_manager.chat(complex_coding_prompt, stream=False)\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        coding_results[model] = {\"response\": response, \"duration\": duration}\n",
    "\n",
    "        print(f\"Response received in {duration:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        coding_results[model] = {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ef53cb",
   "metadata": {},
   "source": [
    "## Comparing Code Quality\n",
    "\n",
    "Let's evaluate the code quality from both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_python_code(text):\n",
    "    \"\"\"Extract Python code blocks from text.\"\"\"\n",
    "    # Match code blocks in markdown format with ```python ... ``` or just ``` ... ```\n",
    "    pattern = r\"```(?:python)?\\n([\\s\\S]*?)\\n```\"\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if matches:\n",
    "        return \"\\n\\n\".join(matches)\n",
    "    return None\n",
    "\n",
    "\n",
    "for model, result in coding_results.items():\n",
    "    if \"error\" not in result:\n",
    "        print(f\"\\n===== {model} Code Solution =====\")\n",
    "        print(f\"Response time: {result['duration']:.2f} seconds\")\n",
    "\n",
    "        # Extract and display the code\n",
    "        code = extract_python_code(result[\"response\"])\n",
    "        if code:\n",
    "            print(\"\\nCode sample:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(code)\n",
    "            print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d86dd",
   "metadata": {},
   "source": [
    "## Testing Model Response to Historical Context\n",
    "\n",
    "Let's see how well each model handles a conversation that requires historical context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_contextual_memory(model_name):\n",
    "    \"\"\"Test how well a model maintains context across multiple turns.\"\"\"\n",
    "    print(f\"\\n===== Testing contextual memory for {model_name} =====\")\n",
    "\n",
    "    chat_manager = create_chat_manager(model_name)\n",
    "\n",
    "    # Multi-turn conversation about a fictional character\n",
    "    prompts = [\n",
    "        \"Let's create a fictional character named Alex who is a software developer with a passion for hiking.\",\n",
    "        \"What programming languages might Alex be proficient in given their career?\",\n",
    "        \"What hiking gear would you recommend for Alex if they're planning a 3-day trek in the mountains?\",\n",
    "        \"Alex is considering a career change to become a park ranger. What transferable skills from software development would be useful?\",\n",
    "    ]\n",
    "\n",
    "    responses = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"\\nTurn {i + 1}: {prompt}\")\n",
    "        try:\n",
    "            response = chat_manager.chat(prompt, stream=False)\n",
    "            print(f\"Response length: {len(response)} characters\")\n",
    "            responses.append({\"prompt\": prompt, \"response\": response})\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "# Test both models\n",
    "gpt_context_test = test_contextual_memory(\"gpt-4.1-nano\")\n",
    "gemini_context_test = test_contextual_memory(\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c4b74",
   "metadata": {},
   "source": [
    "## Conclusion: Model Comparison Summary\n",
    "\n",
    "Let's summarize what we've learned from comparing GPT-4.1-nano and Gemini-2.5-flash models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f826bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_summary(model, results, context_results, coding_result):\n",
    "    \"\"\"Format a summary of model performance.\"\"\"\n",
    "    # Calculate average stats if results are available\n",
    "    if results and all(\"error\" not in r for r in results):\n",
    "        avg_time = sum(r[\"duration\"] for r in results) / len(results)\n",
    "        avg_length = sum(r[\"length\"] for r in results) / len(results)\n",
    "    else:\n",
    "        avg_time = avg_length = \"N/A\"\n",
    "\n",
    "    # Check if coding result is available\n",
    "    if coding_result and \"error\" not in coding_result:\n",
    "        coding_time = coding_result[\"duration\"]\n",
    "        code_quality = \"Available\" if extract_python_code(coding_result[\"response\"]) else \"Not found\"\n",
    "    else:\n",
    "        coding_time = \"N/A\"\n",
    "        code_quality = \"N/A\"\n",
    "\n",
    "    # Check context handling\n",
    "    context_quality = \"Good\" if context_results and len(context_results) >= 3 else \"Limited\"\n",
    "\n",
    "    return f\"\"\"\n",
    "    Model: {model}\n",
    "    Average response time: {avg_time if isinstance(avg_time, str) else f\"{avg_time:.2f}s\"}\n",
    "    Average response length: {avg_length if isinstance(avg_length, str) else f\"{int(avg_length)} chars\"}\n",
    "    Complex coding task time: {coding_time if isinstance(coding_time, str) else f\"{coding_time:.2f}s\"}\n",
    "    Code quality: {code_quality}\n",
    "    Context handling: {context_quality}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# Generate summaries\n",
    "gpt_summary = format_summary(\n",
    "    \"GPT-4.1-nano\",\n",
    "    comparison_results.get(\"gpt-4.1-nano\", []),\n",
    "    gpt_context_test,\n",
    "    coding_results.get(\"gpt-4.1-nano\", {}),\n",
    ")\n",
    "\n",
    "gemini_summary = format_summary(\n",
    "    \"Gemini-2.5-flash\",\n",
    "    comparison_results.get(\"gemini-2.5-flash\", []),\n",
    "    gemini_context_test,\n",
    "    coding_results.get(\"gemini-2.5-flash\", {}),\n",
    ")\n",
    "\n",
    "print(\"===== Model Comparison Summary =====\")\n",
    "print(gpt_summary)\n",
    "print(gemini_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7d0fb",
   "metadata": {},
   "source": [
    "## Final Observations\n",
    "\n",
    "Based on our testing with the cellmage library, both models have their own strengths and weaknesses:\n",
    "\n",
    "**GPT-4.1-nano:**\n",
    "- May excel in coding tasks and technical explanations\n",
    "- Could have stronger contextual memory across conversation turns\n",
    "\n",
    "**Gemini-2.5-flash:**\n",
    "- May excel in creative content generation\n",
    "- Could potentially offer faster response times\n",
    "\n",
    "The cellmage library provides a consistent interface for working with both models, making it easy to switch between them or combine their capabilities for different use cases.\n",
    "\n",
    "When deciding which model to use for a particular application, consider the specific requirements of your task, such as response time constraints, complexity of the problem, and the importance of contextual understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
