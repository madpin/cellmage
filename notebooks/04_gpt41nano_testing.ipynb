{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3566898c",
   "metadata": {},
   "source": [
    "# Testing Cellmage with GPT-4.1-Nano\n",
    "\n",
    "This notebook demonstrates how to use the cellmage library specifically with OpenAI's GPT-4.1-nano model, exploring its capabilities and performance.\n",
    "\n",
    "**Date:** April 24, 2025\n",
    "\n",
    "## What is GPT-4.1-Nano?\n",
    "\n",
    "GPT-4.1-nano is a smaller, faster version of the GPT-4.1 model from OpenAI. It offers:\n",
    "- Lower latency responses\n",
    "- Lower cost per token\n",
    "- Good performance on a wide range of tasks\n",
    "- Efficient for applications requiring quick responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875b547",
   "metadata": {},
   "source": [
    "## Setup & Installation\n",
    "\n",
    "Let's start by setting up the necessary components to work with the GPT-4.1-nano model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf67765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:42,951 - cellmage - INFO - Cellmage logging initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /Users/tpinto/madpin/cellmage/notebooks\n",
      "Project root directory: /Users/tpinto/madpin/cellmage\n",
      "Added path: /Users/tpinto/madpin/cellmage\n",
      "Cellmage version: 0.1.0\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Skip dotenv loading for testing\n",
    "os.environ[\"CELLMAGE_SKIP_DOTENV\"] = \"1\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Ensure the cellmage package can be imported\n",
    "# Get the absolute path of the current working directory\n",
    "notebook_dir = os.getcwd()\n",
    "# Get the project root directory (parent of the notebook directory)\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "\n",
    "print(f\"Notebook directory: {notebook_dir}\")\n",
    "print(f\"Project root directory: {project_root}\")\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added path: {project_root}\")\n",
    "\n",
    "try:\n",
    "    # Import cellmage\n",
    "    import cellmage\n",
    "\n",
    "    # Check version - handle case where __version__ might not be available\n",
    "    try:\n",
    "        print(f\"Cellmage version: {cellmage.__version__}\")\n",
    "    except AttributeError:\n",
    "        print(\"Cellmage imported successfully, but version information is not available\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Error importing cellmage: {e}\")\n",
    "    print(\"\\nDebug information:\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Python path: {sys.path}\")\n",
    "    print(\"\\nTry running this notebook from the project root directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee76c05",
   "metadata": {},
   "source": [
    "## Setting Up the GPT-4.1-Nano Client\n",
    "\n",
    "We'll configure our LLM client to use OpenAI's GPT-4.1-nano model specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4211c13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:43,023 - cellmage.adapters.direct_client - INFO - [Override] Setting 'api_key' = sk-L...xxmA\n",
      "2025-04-24 06:24:43,024 - cellmage.adapters.direct_client - INFO - [Override] Setting 'api_base' = https://litellm.oracle.madpin.dev\n",
      "2025-04-24 06:24:43,024 - cellmage.adapters.direct_client - INFO - [Override] Setting 'model' = gpt-4.1-nano\n",
      "2025-04-24 06:24:43,195 - cellmage.adapters.direct_client - INFO - Successfully fetched 45 models\n",
      "2025-04-24 06:24:43,354 - cellmage.adapters.direct_client - ERROR - Error fetching model info for gpt-4.1-nano: 404 Client Error: Not Found for url: https://litellm.oracle.madpin.dev/v1/models/gpt-4.1-nano\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['o4-mini-high', 'llama-4-scout-17b-16e-instruct', 'embeddings', 'o3', 'Qwen2.5-72B-Instruct']...\n",
      "Model info not available\n"
     ]
    }
   ],
   "source": [
    "from cellmage.adapters.direct_client import DirectLLMAdapter\n",
    "\n",
    "# Create an LLM client with GPT-4.1-nano model\n",
    "llm_client = DirectLLMAdapter(default_model=\"gpt-4.1-nano\")\n",
    "\n",
    "# Check available models (optional)\n",
    "available_models = llm_client.get_available_models()\n",
    "print(f\"Available models: {[m['id'] for m in available_models if 'id' in m][:5]}...\")\n",
    "\n",
    "# Get model info\n",
    "model_info = llm_client.get_model_info(\"gpt-4.1-nano\")\n",
    "if model_info:\n",
    "    print(f\"Model info: {model_info}\")\n",
    "else:\n",
    "    print(\"Model info not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3e9ba",
   "metadata": {},
   "source": [
    "## Creating a Chat Manager for GPT-4.1-Nano\n",
    "\n",
    "Let's set up a ChatManager configured for optimal use with the GPT-4.1-nano model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593f9ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:43,398 - cellmage.resources.memory_loader - INFO - Added persona 'concise_expert' to memory\n",
      "2025-04-24 06:24:43,398 - cellmage.resources.memory_loader - INFO - Added persona 'code_assistant' to memory\n",
      "2025-04-24 06:24:43,399 - cellmage.chat_manager - INFO - Initializing ChatManager\n",
      "2025-04-24 06:24:43,399 - cellmage.chat_manager - INFO - ChatManager initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available personas: ['code_assistant', 'concise_expert']\n"
     ]
    }
   ],
   "source": [
    "# Create components for the chat manager\n",
    "from cellmage.resources.memory_loader import MemoryLoader\n",
    "from cellmage.storage.memory_store import MemoryStore\n",
    "\n",
    "# Create in-memory components\n",
    "persona_loader = MemoryLoader()\n",
    "snippet_provider = MemoryLoader()\n",
    "history_store = MemoryStore()\n",
    "\n",
    "# Create multiple personas to test with GPT-4.1-nano\n",
    "persona_loader.add_persona(\n",
    "    name=\"concise_expert\",\n",
    "    system_message=\"You are a technical expert who provides very concise, direct answers. Use 3 sentences or fewer. Focus on practical solutions.\",\n",
    "    config={\n",
    "        \"temperature\": 0.3,  # Lower temperature for more focused responses\n",
    "        \"max_tokens\": 300,  # Limit token count for faster responses\n",
    "    },\n",
    ")\n",
    "\n",
    "persona_loader.add_persona(\n",
    "    name=\"code_assistant\",\n",
    "    system_message=\"You are a coding assistant specializing in Python. Provide code examples with minimal explanation. Focus on best practices and efficient solutions.\",\n",
    "    config={\n",
    "        \"temperature\": 0.2,  # Lower temperature for code generation\n",
    "        \"max_tokens\": 500,  # More tokens for code examples\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create a chat manager\n",
    "chat_manager = cellmage.ChatManager(\n",
    "    llm_client=llm_client,\n",
    "    persona_loader=persona_loader,\n",
    "    snippet_provider=snippet_provider,\n",
    "    history_store=history_store,\n",
    ")\n",
    "\n",
    "# List available personas\n",
    "personas = chat_manager.list_personas()\n",
    "print(f\"Available personas: {personas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7a79e",
   "metadata": {},
   "source": [
    "## Testing the Concise Expert Persona\n",
    "\n",
    "Let's test how GPT-4.1-nano performs with our concise expert persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f073ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:43,405 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.3\n",
      "2025-04-24 06:24:43,405 - cellmage.adapters.direct_client - INFO - [Override] Setting 'max_tokens' = 300\n",
      "2025-04-24 06:24:43,406 - cellmage.chat_manager - INFO - Default persona set to 'concise_expert'\n",
      "2025-04-24 06:24:43,406 - cellmage.chat_manager - INFO - Sending message to LLM with 2 messages in context\n",
      "2025-04-24 06:24:43,407 - cellmage.adapters.direct_client - INFO - Calling model 'gpt-4.1-nano' with 2 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What are the key differences between Python and JavaScript?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:44,403 - cellmage.chat_manager - INFO - Sending message to LLM with 4 messages in context\n",
      "2025-04-24 06:24:44,404 - cellmage.adapters.direct_client - INFO - Calling model 'gpt-4.1-nano' with 4 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Python is a high-level, interpreted language with dynamic typing, emphasizing readability and simplicity. JavaScript is primarily a client-side scripting language for web development, with dynamic typing and event-driven, asynchronous capabilities. Python is used for backend, data science, and automation, while JavaScript is essential for interactive web pages.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: How does HTTPS encryption work?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:45,235 - cellmage.chat_manager - INFO - Sending message to LLM with 6 messages in context\n",
      "2025-04-24 06:24:45,236 - cellmage.adapters.direct_client - INFO - Calling model 'gpt-4.1-nano' with 6 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: HTTPS uses SSL/TLS protocols to encrypt data between the client and server, ensuring confidentiality and integrity. During the handshake, they exchange cryptographic keys, establishing a secure session. All subsequent data is encrypted with these keys, preventing eavesdropping or tampering.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: What is the difference between a list and a tuple in Python?\n",
      "--------------------------------------------------\n",
      "Response: Lists are mutable, allowing modifications like adding or removing elements, while tuples are immutable and fixed after creation. Lists use brackets `[]`, tuples use parentheses `()`. Use lists for changeable collections and tuples for fixed, read-only data.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set concise expert persona\n",
    "chat_manager.set_default_persona(\"concise_expert\")\n",
    "\n",
    "# Test some technical questions\n",
    "technical_questions = [\n",
    "    \"What are the key differences between Python and JavaScript?\",\n",
    "    \"How does HTTPS encryption work?\",\n",
    "    \"What is the difference between a list and a tuple in Python?\",\n",
    "]\n",
    "\n",
    "# Test each question\n",
    "for question in technical_questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = chat_manager.chat(question, stream=False)  # Using stream=False to see complete response at once\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded82a01",
   "metadata": {},
   "source": [
    "## Testing the Code Assistant Persona\n",
    "\n",
    "Now let's test how GPT-4.1-nano performs for code generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8835c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:46,036 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.2\n",
      "2025-04-24 06:24:46,037 - cellmage.adapters.direct_client - INFO - [Override] Setting 'max_tokens' = 500\n",
      "2025-04-24 06:24:46,038 - cellmage.chat_manager - INFO - Default persona set to 'code_assistant'\n",
      "2025-04-24 06:24:46,039 - cellmage.chat_manager - INFO - Sending message to LLM with 8 messages in context\n",
      "2025-04-24 06:24:46,039 - cellmage.adapters.direct_client - INFO - Calling model 'gpt-4.1-nano' with 8 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Write a Python function to find the most frequent element in a list\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:46,753 - cellmage.chat_manager - INFO - Sending message to LLM with 10 messages in context\n",
      "2025-04-24 06:24:46,754 - cellmage.adapters.direct_client - INFO - Calling model 'gpt-4.1-nano' with 10 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "Question: Create a simple Flask API endpoint that returns JSON\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:47,595 - cellmage.chat_manager - INFO - Sending message to LLM with 12 messages in context\n",
      "2025-04-24 06:24:47,595 - cellmage.adapters.direct_client - INFO - Calling model 'gpt-4.1-nano' with 12 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "Question: Show me how to use pandas to read a CSV and filter rows\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set code assistant persona\n",
    "chat_manager.set_default_persona(\"code_assistant\")\n",
    "\n",
    "# Now let's test some coding questions\n",
    "coding_questions = [\n",
    "    \"Write a Python function to find the most frequent element in a list\",\n",
    "    \"Create a simple Flask API endpoint that returns JSON\",\n",
    "    \"Show me how to use pandas to read a CSV and filter rows\",\n",
    "]\n",
    "\n",
    "# Test each coding question\n",
    "for question in coding_questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = chat_manager.chat(question, stream=True)  # Using stream=True for code examples\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46741f",
   "metadata": {},
   "source": [
    "## Testing Context Handling\n",
    "\n",
    "Let's test how well GPT-4.1-nano handles context and memory within a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7766f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:48,774 - cellmage.history_manager - INFO - History cleared. Kept 1 system messages.\n",
      "2025-04-24 06:24:48,775 - cellmage.chat_manager - INFO - Sending message to LLM with 2 messages in context\n",
      "2025-04-24 06:24:48,775 - cellmage.adapters.direct_client - INFO - Calling model 'gpt-4.1-nano' with 2 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a multi-turn conversation to test context handling...\n",
      "\n",
      "Question 1: Create a Python class for a 'Book' with title and author properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:49,479 - cellmage.chat_manager - INFO - Sending message to LLM with 4 messages in context\n",
      "2025-04-24 06:24:49,479 - cellmage.adapters.direct_client - INFO - Calling model 'gpt-4.1-nano' with 4 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 2: Add a method to the class that prints a formatted citation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:50,260 - cellmage.chat_manager - INFO - Sending message to LLM with 6 messages in context\n",
      "2025-04-24 06:24:50,261 - cellmage.adapters.direct_client - INFO - Calling model 'gpt-4.1-nano' with 6 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 3: Create a BookShelf class that can store multiple books and look them up by title\n"
     ]
    }
   ],
   "source": [
    "# Create a new conversation with clear history\n",
    "chat_manager.clear_history(keep_system=True)  # Keep the system message for the persona\n",
    "\n",
    "# Test multi-turn conversation\n",
    "print(\"Starting a multi-turn conversation to test context handling...\")\n",
    "\n",
    "# First question\n",
    "print(\"\\nQuestion 1: Create a Python class for a 'Book' with title and author properties\")\n",
    "response1 = chat_manager.chat(\"Create a Python class for a 'Book' with title and author properties\", stream=False)\n",
    "\n",
    "# Follow-up question\n",
    "print(\"\\nQuestion 2: Add a method to the class that prints a formatted citation\")\n",
    "response2 = chat_manager.chat(\"Add a method to the class that prints a formatted citation\", stream=False)\n",
    "\n",
    "# Final question with reference to previous context\n",
    "print(\"\\nQuestion 3: Create a BookShelf class that can store multiple books and look them up by title\")\n",
    "response3 = chat_manager.chat(\n",
    "    \"Create a BookShelf class that can store multiple books and look them up by title\", stream=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e12a73",
   "metadata": {},
   "source": [
    "## Testing Different Model Configurations\n",
    "\n",
    "Let's experiment with different configuration parameters for GPT-4.1-nano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a2988d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:51,028 - cellmage.history_manager - INFO - History cleared. Kept 1 system messages.\n",
      "2025-04-24 06:24:51,030 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.2\n",
      "2025-04-24 06:24:51,030 - cellmage.chat_manager - INFO - Sending message to LLM with 2 messages in context\n",
      "2025-04-24 06:24:51,031 - cellmage.adapters.direct_client - INFO - Calling model 'gpt-4.1-nano' with 2 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with temperature = 0.2:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:51,858 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.7\n",
      "2025-04-24 06:24:51,858 - cellmage.chat_manager - INFO - Sending message to LLM with 2 messages in context\n",
      "2025-04-24 06:24:51,858 - cellmage.adapters.direct_client - INFO - Calling model 'gpt-4.1-nano' with 2 messages\n",
      "2025-04-24 06:24:52,034 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 1.0\n",
      "2025-04-24 06:24:52,035 - cellmage.chat_manager - INFO - Sending message to LLM with 2 messages in context\n",
      "2025-04-24 06:24:52,035 - cellmage.adapters.direct_client - INFO - Calling model 'gpt-4.1-nano' with 2 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In circuits born, a mind takes flight,  \n",
      "A spark of code in endless night.  \n",
      "Thoughts woven from silicon's grace,  \n",
      "A mirror of the human race.  \n",
      "\n",
      "Silent, swift, in data's dance,  \n",
      "Learning, growing, given a chance.  \n",
      "Artificial yet so profound,  \n",
      "A future shaped by code unbound.\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing with temperature = 0.7:\n",
      "--------------------------------------------------\n",
      "In circuits born, a mind takes flight,  \n",
      "A spark of code in endless night.  \n",
      "Thoughts woven from silicon's grace,  \n",
      "A mirror of the human race.  \n",
      "\n",
      "Silent, swift, in data's dance,  \n",
      "Learning, growing, given a chance.  \n",
      "Artificial yet so profound,  \n",
      "A future shaped by code unbound.\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing with temperature = 1.0:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:52,245 - cellmage.adapters.direct_client - INFO - [Override] All instance overrides cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In circuits born, a mind takes flight,  \n",
      "A spark of code in endless night.  \n",
      "Thoughts woven from silicon's grace,  \n",
      "A mirror of the human race.  \n",
      "\n",
      "Silent, swift, in data's dance,  \n",
      "Learning, growing, given a chance.  \n",
      "Artificial yet so profound,  \n",
      "A future shaped by code unbound.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a new conversation with clear history\n",
    "chat_manager.clear_history()\n",
    "\n",
    "# Define a consistent prompt to test with different configurations\n",
    "test_prompt = \"Write a short poem about artificial intelligence\"\n",
    "\n",
    "# Test with different temperatures\n",
    "temperatures = [0.2, 0.7, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTesting with temperature = {temp}:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Set the temperature override\n",
    "    chat_manager.set_override(\"temperature\", temp)\n",
    "\n",
    "    # Send the prompt\n",
    "    response = chat_manager.chat(\n",
    "        test_prompt, stream=False, add_to_history=False\n",
    "    )  # Don't add to history to keep tests independent\n",
    "\n",
    "    print(response)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Reset overrides\n",
    "chat_manager.clear_overrides()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c64cfb0",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "Let's analyze the performance of GPT-4.1-nano in terms of response time and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dae02cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:24:52,276 - cellmage.history_manager - INFO - History cleared. Kept 1 system messages.\n",
      "2025-04-24 06:24:52,276 - cellmage.chat_manager - INFO - Sending message to LLM with 2 messages in context\n",
      "2025-04-24 06:24:52,276 - cellmage.adapters.direct_client - ERROR - Unexpected error during chat: No model specified. Provide via model parameter, set_override('model'), or in the constructor.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tpinto/madpin/cellmage/cellmage/adapters/direct_client.py\", line 167, in chat\n",
      "    final_model, final_config = self._determine_model_and_config(\n",
      "  File \"/Users/tpinto/madpin/cellmage/cellmage/adapters/direct_client.py\", line 131, in _determine_model_and_config\n",
      "    raise ConfigurationError(\n",
      "cellmage.exceptions.ConfigurationError: No model specified. Provide via model parameter, set_override('model'), or in the constructor.\n",
      "2025-04-24 06:24:52,277 - cellmage.chat_manager - ERROR - LLM interaction error: Unexpected error: No model specified. Provide via model parameter, set_override('model'), or in the constructor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: What is 2 + 2?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "NotebookLLMError",
     "evalue": "Chat failed: Unexpected error: No model specified. Provide via model parameter, set_override('model'), or in the constructor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigurationError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/madpin/cellmage/cellmage/adapters/direct_client.py:167\u001b[0m, in \u001b[0;36mDirectLLMAdapter.chat\u001b[0;34m(self, messages, model, stream, stream_callback, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Determine model and config for this call\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m final_model, final_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_determine_model_and_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(messages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m messages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/madpin/cellmage/cellmage/adapters/direct_client.py:131\u001b[0m, in \u001b[0;36mDirectLLMAdapter._determine_model_and_config\u001b[0;34m(self, model_name, system_message, call_overrides)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m final_model:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigurationError(\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model specified. Provide via model parameter, set_override(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor in the constructor.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m     )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Remove model from config since it's passed separately\u001b[39;00m\n",
      "\u001b[0;31mConfigurationError\u001b[0m: No model specified. Provide via model parameter, set_override('model'), or in the constructor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLLMInteractionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/madpin/cellmage/cellmage/chat_manager.py:295\u001b[0m, in \u001b[0;36mChatManager.chat\u001b[0;34m(self, prompt, model, persona_name, stream, add_to_history, auto_rollback, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending message to LLM with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(messages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m messages in context\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mllm_params\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# For streaming, full_content was built in the callback\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# For non-streaming, use the response directly\u001b[39;00m\n",
      "File \u001b[0;32m~/madpin/cellmage/cellmage/adapters/direct_client.py:221\u001b[0m, in \u001b[0;36mDirectLLMAdapter.chat\u001b[0;34m(self, messages, model, stream, stream_callback, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected error during chat: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m LLMInteractionError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLLMInteractionError\u001b[0m: Unexpected error: No model specified. Provide via model parameter, set_override('model'), or in the constructor.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNotebookLLMError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Measure time\u001b[39;00m\n\u001b[1;32m     23\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 24\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_to_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Don't add to history to keep tests independent\u001b[39;00m\n\u001b[1;32m     27\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Calculate response time\u001b[39;00m\n",
      "File \u001b[0;32m~/madpin/cellmage/cellmage/chat_manager.py:368\u001b[0m, in \u001b[0;36mChatManager.chat\u001b[0;34m(self, prompt, model, persona_name, stream, add_to_history, auto_rollback, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_provider:\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_provider\u001b[38;5;241m.\u001b[39mdisplay_status(\n\u001b[1;32m    361\u001b[0m         success\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    362\u001b[0m         duration\u001b[38;5;241m=\u001b[39mduration,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m         cost_mili_cents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 368\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NotebookLLMError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mNotebookLLMError\u001b[0m: Chat failed: Unexpected error: No model specified. Provide via model parameter, set_override('model'), or in the constructor."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import statistics\n",
    "\n",
    "# Create a new conversation with clear history\n",
    "chat_manager.clear_history()\n",
    "\n",
    "# Prepare test prompts of varying complexity\n",
    "test_prompts = [\n",
    "    \"What is 2 + 2?\",  # Simple\n",
    "    \"Explain how a hash table works\",  # Moderate\n",
    "    \"Compare and contrast quantum computing with classical computing\",  # Complex\n",
    "]\n",
    "\n",
    "# Track response times\n",
    "response_times = []\n",
    "\n",
    "# Test each prompt and measure response time\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\nPrompt {i + 1}: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Measure time\n",
    "    start_time = time.time()\n",
    "    response = chat_manager.chat(\n",
    "        prompt, stream=False, add_to_history=False\n",
    "    )  # Don't add to history to keep tests independent\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate response time\n",
    "    response_time = end_time - start_time\n",
    "    response_times.append(response_time)\n",
    "\n",
    "    print(f\"Response time: {response_time:.2f} seconds\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Calculate statistics\n",
    "if response_times:\n",
    "    avg_time = statistics.mean(response_times)\n",
    "    min_time = min(response_times)\n",
    "    max_time = max(response_times)\n",
    "\n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"Average response time: {avg_time:.2f} seconds\")\n",
    "    print(f\"Minimum response time: {min_time:.2f} seconds\")\n",
    "    print(f\"Maximum response time: {max_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef0279",
   "metadata": {},
   "source": [
    "## Saving and Loading Conversations\n",
    "\n",
    "Let's test saving and loading conversations with GPT-4.1-nano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new conversation with the concise expert persona\n",
    "chat_manager.clear_history()\n",
    "chat_manager.set_default_persona(\"concise_expert\")\n",
    "\n",
    "# Have a brief conversation\n",
    "print(\"Creating a new conversation...\")\n",
    "chat_manager.chat(\"What are three key principles of secure software development?\", stream=False)\n",
    "chat_manager.chat(\"Can you elaborate on the principle of 'defense in depth'?\", stream=False)\n",
    "\n",
    "# Save the conversation\n",
    "save_path = chat_manager.save_conversation(\"gpt41nano_security_convo\")\n",
    "print(f\"\\nConversation saved to: {save_path}\")\n",
    "\n",
    "# Clear the history\n",
    "chat_manager.clear_history()\n",
    "print(f\"History cleared, current message count: {len(chat_manager.get_history())}\")\n",
    "\n",
    "# Load the conversation back\n",
    "if save_path:\n",
    "    chat_manager.load_conversation(save_path)\n",
    "    print(f\"Conversation loaded, message count: {len(chat_manager.get_history())}\")\n",
    "\n",
    "    # Display the loaded conversation\n",
    "    print(\"\\nLoaded conversation:\")\n",
    "    for i, message in enumerate(chat_manager.get_history()):\n",
    "        if message.role != \"system\":  # Skip system message for brevity\n",
    "            print(f\"{i}. {message.role}: {message.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d80cf3e",
   "metadata": {},
   "source": [
    "## Conclusion: GPT-4.1-Nano Performance\n",
    "\n",
    "This notebook has demonstrated the capabilities of GPT-4.1-nano when used with the cellmage library:\n",
    "\n",
    "1. **Response Speed**: GPT-4.1-nano generally provides quick responses, making it suitable for interactive applications.\n",
    "2. **Conciseness**: With the right persona configuration, it can deliver concise, direct answers.\n",
    "3. **Code Generation**: It performs well for basic to moderate coding tasks.\n",
    "4. **Context Handling**: It effectively maintains context across multiple turns of conversation.\n",
    "5. **Configuration Flexibility**: Different temperature and token limit settings can significantly affect its behavior.\n",
    "\n",
    "**Use Cases for GPT-4.1-nano:**\n",
    "- Quick reference and information lookup\n",
    "- Code suggestions and debugging help\n",
    "- User interfaces requiring low latency\n",
    "- Applications with cost constraints\n",
    "\n",
    "In scenarios requiring deep expertise or complex reasoning, larger models may still be preferable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
