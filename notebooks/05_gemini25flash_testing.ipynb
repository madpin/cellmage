{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c28f493",
   "metadata": {},
   "source": [
    "# Testing Cellmage with Gemini 2.5 Flash\n",
    "\n",
    "This notebook demonstrates how to use the cellmage library with Google's Gemini 2.5 Flash model, exploring its capabilities and performance characteristics.\n",
    "\n",
    "**Date:** April 24, 2025\n",
    "\n",
    "## What is Gemini 2.5 Flash?\n",
    "\n",
    "Gemini 2.5 Flash is a lightweight version of Google's Gemini 2.5 model designed for low-latency applications. It offers:\n",
    "- Very fast response times\n",
    "- Cost-effective inference\n",
    "- Strong performance on a range of tasks\n",
    "- Multimodal capabilities\n",
    "- Effective tool use and function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25b390",
   "metadata": {},
   "source": [
    "## Setup & Installation\n",
    "\n",
    "Let's set up our environment to work with the Gemini 2.5 Flash model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2032aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:25:21,292 - cellmage - INFO - Cellmage logging initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /Users/tpinto/madpin/cellmage/notebooks\n",
      "Project root directory: /Users/tpinto/madpin/cellmage\n",
      "Added path: /Users/tpinto/madpin/cellmage\n",
      "Cellmage version: 0.1.0\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Skip dotenv loading for testing\n",
    "os.environ[\"CELLMAGE_SKIP_DOTENV\"] = \"1\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Ensure the cellmage package can be imported\n",
    "# Get the absolute path of the current working directory\n",
    "notebook_dir = os.getcwd()\n",
    "# Get the project root directory (parent of the notebook directory)\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "\n",
    "print(f\"Notebook directory: {notebook_dir}\")\n",
    "print(f\"Project root directory: {project_root}\")\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added path: {project_root}\")\n",
    "\n",
    "try:\n",
    "    # Import cellmage\n",
    "    import cellmage\n",
    "\n",
    "    # Check version - handle case where __version__ might not be available\n",
    "    try:\n",
    "        print(f\"Cellmage version: {cellmage.__version__}\")\n",
    "    except AttributeError:\n",
    "        print(\"Cellmage imported successfully, but version information is not available\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Error importing cellmage: {e}\")\n",
    "    print(\"\\nDebug information:\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Python path: {sys.path}\")\n",
    "    print(\"\\nTry running this notebook from the project root directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d56b95",
   "metadata": {},
   "source": [
    "## Setting Up the Gemini 2.5 Flash Client\n",
    "\n",
    "We'll configure our LLM client to use Google's Gemini 2.5 Flash model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a804d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:25:21,410 - cellmage.adapters.direct_client - INFO - [Override] Setting 'api_key' = sk-L...xxmA\n",
      "2025-04-24 06:25:21,410 - cellmage.adapters.direct_client - INFO - [Override] Setting 'api_base' = https://litellm.oracle.madpin.dev\n",
      "2025-04-24 06:25:21,411 - cellmage.adapters.direct_client - INFO - [Override] Setting 'model' = gemini-2.5-flash\n",
      "2025-04-24 06:25:21,624 - cellmage.adapters.direct_client - INFO - Successfully fetched 45 models\n",
      "2025-04-24 06:25:21,770 - cellmage.adapters.direct_client - ERROR - Error fetching model info for gemini-2.5-flash: 404 Client Error: Not Found for url: https://litellm.oracle.madpin.dev/v1/models/gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['o4-mini-high', 'llama-4-scout-17b-16e-instruct', 'embeddings', 'o3', 'Qwen2.5-72B-Instruct']...\n",
      "Model info not available\n"
     ]
    }
   ],
   "source": [
    "from cellmage.adapters.direct_client import DirectLLMAdapter\n",
    "\n",
    "# Create an LLM client with Gemini 2.5 Flash model\n",
    "llm_client = DirectLLMAdapter(default_model=\"gemini-2.5-flash\")\n",
    "\n",
    "# You'll need to ensure your API key is set up in your environment\n",
    "# For Google: export GOOGLE_API_KEY=your_api_key\n",
    "\n",
    "# Check available models (optional)\n",
    "available_models = llm_client.get_available_models()\n",
    "print(f\"Available models: {[m['id'] for m in available_models if 'id' in m][:5]}...\")\n",
    "\n",
    "# Get model info\n",
    "model_info = llm_client.get_model_info(\"gemini-2.5-flash\")\n",
    "if model_info:\n",
    "    print(f\"Model info: {model_info}\")\n",
    "else:\n",
    "    print(\"Model info not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1da9be",
   "metadata": {},
   "source": [
    "## Creating a Chat Manager for Gemini 2.5 Flash\n",
    "\n",
    "Let's set up a ChatManager specifically configured for Gemini 2.5 Flash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59fa02ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:25:21,814 - cellmage.resources.memory_loader - INFO - Added persona 'creative_writer' to memory\n",
      "2025-04-24 06:25:21,814 - cellmage.resources.memory_loader - INFO - Added persona 'data_analyst' to memory\n",
      "2025-04-24 06:25:21,815 - cellmage.chat_manager - INFO - Initializing ChatManager\n",
      "2025-04-24 06:25:21,815 - cellmage.chat_manager - INFO - ChatManager initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available personas: ['creative_writer', 'data_analyst']\n"
     ]
    }
   ],
   "source": [
    "# Create components for the chat manager\n",
    "from cellmage.resources.memory_loader import MemoryLoader\n",
    "from cellmage.storage.memory_store import MemoryStore\n",
    "\n",
    "# Create in-memory components\n",
    "persona_loader = MemoryLoader()\n",
    "snippet_provider = MemoryLoader()\n",
    "history_store = MemoryStore()\n",
    "\n",
    "# Create specialized personas for Gemini 2.5 Flash\n",
    "persona_loader.add_persona(\n",
    "    name=\"creative_writer\",\n",
    "    system_message=\"You are a creative writing assistant with expertise in storytelling, poetry, and creative content generation. Provide imaginative and engaging responses.\",\n",
    "    config={\n",
    "        \"temperature\": 0.8,  # Higher temperature for creativity\n",
    "        \"top_p\": 0.95,  # Diverse sampling\n",
    "    },\n",
    ")\n",
    "\n",
    "persona_loader.add_persona(\n",
    "    name=\"data_analyst\",\n",
    "    system_message=\"You are a data analysis expert who provides insights on data processing, visualization, and statistical analysis using Python libraries like pandas, matplotlib, and scipy. Focus on clear, structured explanations.\",\n",
    "    config={\n",
    "        \"temperature\": 0.3,  # Lower temperature for technical content\n",
    "        \"top_k\": 40,  # Narrower sampling for technical precision\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create a chat manager\n",
    "chat_manager = cellmage.ChatManager(\n",
    "    llm_client=llm_client,\n",
    "    persona_loader=persona_loader,\n",
    "    snippet_provider=snippet_provider,\n",
    "    history_store=history_store,\n",
    ")\n",
    "\n",
    "# List available personas\n",
    "personas = chat_manager.list_personas()\n",
    "print(f\"Available personas: {personas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9adc013",
   "metadata": {},
   "source": [
    "## Testing Creative Writing Capabilities\n",
    "\n",
    "Let's test how Gemini 2.5 Flash performs with creative writing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1c7491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:25:21,820 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.8\n",
      "2025-04-24 06:25:21,821 - cellmage.adapters.direct_client - INFO - [Override] Setting 'top_p' = 0.95\n",
      "2025-04-24 06:25:21,821 - cellmage.chat_manager - INFO - Default persona set to 'creative_writer'\n",
      "2025-04-24 06:25:21,822 - cellmage.chat_manager - INFO - Sending message to LLM with 2 messages in context\n",
      "2025-04-24 06:25:21,823 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 2 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Write a short poem about the changing seasons\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:25:27,524 - cellmage.chat_manager - INFO - Sending message to LLM with 4 messages in context\n",
      "2025-04-24 06:25:27,524 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 4 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "Prompt: Create a brief story opening that takes place in a futuristic underwater city\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:25:33,369 - cellmage.chat_manager - INFO - Sending message to LLM with 6 messages in context\n",
      "2025-04-24 06:25:33,369 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 6 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "Prompt: Describe a fantasy creature that combines elements of three different animals\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set creative writer persona\n",
    "chat_manager.set_default_persona(\"creative_writer\")\n",
    "\n",
    "# Test creative writing prompts\n",
    "creative_prompts = [\n",
    "    \"Write a short poem about the changing seasons\",\n",
    "    \"Create a brief story opening that takes place in a futuristic underwater city\",\n",
    "    \"Describe a fantasy creature that combines elements of three different animals\",\n",
    "]\n",
    "\n",
    "# Test each prompt\n",
    "for prompt in creative_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = chat_manager.chat(prompt, stream=True)  # Using streaming for creative content\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22652e76",
   "metadata": {},
   "source": [
    "## Testing Data Analysis Capabilities\n",
    "\n",
    "Now let's test how Gemini 2.5 Flash handles data analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0737a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:25:47,410 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.3\n",
      "2025-04-24 06:25:47,411 - cellmage.adapters.direct_client - INFO - [Override] Setting 'top_k' = 40\n",
      "2025-04-24 06:25:47,411 - cellmage.chat_manager - INFO - Default persona set to 'data_analyst'\n",
      "2025-04-24 06:25:47,412 - cellmage.resources.memory_loader - INFO - Added snippet 'pandas_example' to memory\n",
      "2025-04-24 06:25:47,412 - cellmage.chat_manager - INFO - Added snippet 'pandas_example' as system message\n",
      "2025-04-24 06:25:47,413 - cellmage.chat_manager - INFO - Sending message to LLM with 9 messages in context\n",
      "2025-04-24 06:25:47,415 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 9 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: How would I calculate the average value by category in this dataframe?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:26:01,600 - cellmage.chat_manager - INFO - Sending message to LLM with 11 messages in context\n",
      "2025-04-24 06:26:01,601 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 11 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "Question: Show me how to create a time series plot of the 'value' column over time\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:26:07,513 - cellmage.chat_manager - INFO - Sending message to LLM with 13 messages in context\n",
      "2025-04-24 06:26:07,513 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 13 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "Question: What's the best way to detect and handle outliers in the 'value' column?\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set data analyst persona\n",
    "chat_manager.set_default_persona(\"data_analyst\")\n",
    "\n",
    "# Add a pandas code snippet for context\n",
    "snippet_provider.add_snippet(\n",
    "    name=\"pandas_example\",\n",
    "    content=\"\"\"```python\n",
    "# Sample pandas dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    'date': pd.date_range('2023-01-01', periods=10),\n",
    "    'value': np.random.normal(100, 15, 10),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 10)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n",
    "```\"\"\",\n",
    ")\n",
    "\n",
    "# Add the snippet to the conversation\n",
    "chat_manager.add_snippet(\"pandas_example\")\n",
    "\n",
    "# Test data analysis questions\n",
    "data_questions = [\n",
    "    \"How would I calculate the average value by category in this dataframe?\",\n",
    "    \"Show me how to create a time series plot of the 'value' column over time\",\n",
    "    \"What's the best way to detect and handle outliers in the 'value' column?\",\n",
    "]\n",
    "\n",
    "# Test each question\n",
    "for question in data_questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = chat_manager.chat(question, stream=True)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7341a",
   "metadata": {},
   "source": [
    "## Testing Multimodal Capabilities\n",
    "\n",
    "Gemini 2.5 Flash has multimodal capabilities. Let's test how it handles image descriptions and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d997bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal testing with Gemini 2.5 Flash:\n",
      "Note: This requires extending cellmage to support image inputs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Create an image description\\nimage_description = \"[This is a placeholder for an image of a data visualization showing a scatter plot with clusters of data points in different colors.]\"\\n\\n# Add the image description as a special message\\nchat_manager.chat(f\"Please analyze this data visualization: {image_description}\", stream=True)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: Multimodal support requires specific setup with the LiteLLM adapter\n",
    "# This is a demonstration of how it would work if properly configured\n",
    "\n",
    "# For a proper implementation, you would need to extend the LiteLLM adapter\n",
    "# to handle image inputs properly\n",
    "\n",
    "print(\"Multimodal testing with Gemini 2.5 Flash:\")\n",
    "print(\"Note: This requires extending cellmage to support image inputs\")\n",
    "\n",
    "# Example of what the implementation might look like:\n",
    "\"\"\"\n",
    "# Create an image description\n",
    "image_description = \"[This is a placeholder for an image of a data visualization showing a scatter plot with clusters of data points in different colors.]\"\n",
    "\n",
    "# Add the image description as a special message\n",
    "chat_manager.chat(f\"Please analyze this data visualization: {image_description}\", stream=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab649ea6",
   "metadata": {},
   "source": [
    "## Testing Function Calling\n",
    "\n",
    "Gemini 2.5 Flash has strong function calling capabilities. Let's test how it works with cellmage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f91eb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calling with Gemini 2.5 Flash:\n",
      "Note: This requires extending cellmage to support function definitions and execution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Extended LiteLLM adapter with function calling support\\nllm_client_with_functions = LiteLLMAdapter(\\n    model=\"gemini-2.5-flash\",\\n    functions=function_definitions\\n)\\n\\n# Create a new chat manager with function support\\nfunction_chat_manager = cellmage.ChatManager(\\n    llm_client=llm_client_with_functions,\\n    persona_loader=persona_loader,\\n    snippet_provider=snippet_provider,\\n    history_store=history_store\\n)\\n\\n# Test function calling\\nresponse = function_chat_manager.chat(\"What\\'s the weather like in San Francisco?\", stream=False)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: Function calling requires specific setup with the LiteLLM adapter\n",
    "# This is a demonstration of how it would work conceptually\n",
    "\n",
    "# Example function definition schema\n",
    "function_definitions = [\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get the current weather for a location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\", \"description\": \"The city and state or country\"},\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The unit of temperature\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Function calling with Gemini 2.5 Flash:\")\n",
    "print(\"Note: This requires extending cellmage to support function definitions and execution\")\n",
    "\n",
    "# Example of what the implementation might look like:\n",
    "\"\"\"\n",
    "# Extended LiteLLM adapter with function calling support\n",
    "llm_client_with_functions = LiteLLMAdapter(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    functions=function_definitions\n",
    ")\n",
    "\n",
    "# Create a new chat manager with function support\n",
    "function_chat_manager = cellmage.ChatManager(\n",
    "    llm_client=llm_client_with_functions,\n",
    "    persona_loader=persona_loader,\n",
    "    snippet_provider=snippet_provider,\n",
    "    history_store=history_store\n",
    ")\n",
    "\n",
    "# Test function calling\n",
    "response = function_chat_manager.chat(\"What's the weather like in San Francisco?\", stream=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a611fd0",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "Let's analyze the performance of Gemini 2.5 Flash in terms of response time and token efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e385c6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:26:26,691 - cellmage.history_manager - INFO - History cleared. Kept 2 system messages.\n",
      "2025-04-24 06:26:26,692 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.3\n",
      "2025-04-24 06:26:26,693 - cellmage.adapters.direct_client - INFO - [Override] Setting 'top_k' = 40\n",
      "2025-04-24 06:26:26,693 - cellmage.chat_manager - INFO - Default persona set to 'data_analyst'\n",
      "2025-04-24 06:26:26,694 - cellmage.chat_manager - INFO - Sending message to LLM with 2 messages in context\n",
      "2025-04-24 06:26:26,694 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 2 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: What is the capital of France?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:26:29,268 - cellmage.chat_manager - INFO - Sending message to LLM with 2 messages in context\n",
      "2025-04-24 06:26:29,268 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 2 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 2.57 seconds\n",
      "Response length: 35 characters\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt 2: Explain how neural networks learn\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:26:44,610 - cellmage.chat_manager - INFO - Sending message to LLM with 2 messages in context\n",
      "2025-04-24 06:26:44,614 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 2 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 15.34 seconds\n",
      "Response length: 5282 characters\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt 3: Compare and contrast different sorting algorithms in terms of time complexity and use cases\n",
      "--------------------------------------------------\n",
      "Response time: 31.13 seconds\n",
      "Response length: 14702 characters\n",
      "--------------------------------------------------\n",
      "\n",
      "Performance Summary:\n",
      "Average response time: 16.35 seconds\n",
      "Average response length: 6673.00 characters\n",
      "Characters per second: 408.17\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import statistics\n",
    "\n",
    "# Create a new conversation with clear history\n",
    "chat_manager.clear_history()\n",
    "\n",
    "# Prepare test prompts of varying complexity\n",
    "test_prompts = [\n",
    "    \"What is the capital of France?\",  # Simple fact\n",
    "    \"Explain how neural networks learn\",  # Moderate explanation\n",
    "    \"Compare and contrast different sorting algorithms in terms of time complexity and use cases\",  # Complex analysis\n",
    "]\n",
    "\n",
    "# Set a consistent persona\n",
    "chat_manager.set_default_persona(\"data_analyst\")\n",
    "\n",
    "# Track response times\n",
    "response_times = []\n",
    "response_lengths = []\n",
    "\n",
    "# Test each prompt and measure response time\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\nPrompt {i + 1}: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Measure time\n",
    "    start_time = time.time()\n",
    "    response = chat_manager.chat(\n",
    "        prompt, stream=False, add_to_history=False\n",
    "    )  # Don't add to history to keep tests independent\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate response time and length\n",
    "    response_time = end_time - start_time\n",
    "    response_times.append(response_time)\n",
    "    response_lengths.append(len(response))\n",
    "\n",
    "    print(f\"Response time: {response_time:.2f} seconds\")\n",
    "    print(f\"Response length: {len(response)} characters\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Calculate statistics\n",
    "if response_times and response_lengths:\n",
    "    avg_time = statistics.mean(response_times)\n",
    "    avg_length = statistics.mean(response_lengths)\n",
    "    chars_per_second = sum(response_lengths) / sum(response_times)\n",
    "\n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"Average response time: {avg_time:.2f} seconds\")\n",
    "    print(f\"Average response length: {avg_length:.2f} characters\")\n",
    "    print(f\"Characters per second: {chars_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab0a29",
   "metadata": {},
   "source": [
    "## Comparing Responses Across Different Prompts\n",
    "\n",
    "Let's test how Gemini 2.5 Flash handles the same prompts with different personas to evaluate consistency and adaptability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d683057a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:27:15,754 - cellmage.history_manager - INFO - History cleared. Kept 2 system messages.\n",
      "2025-04-24 06:27:15,761 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.8\n",
      "2025-04-24 06:27:15,762 - cellmage.adapters.direct_client - INFO - [Override] Setting 'top_p' = 0.95\n",
      "2025-04-24 06:27:15,763 - cellmage.chat_manager - INFO - Default persona set to 'creative_writer'\n",
      "2025-04-24 06:27:15,767 - cellmage.history_manager - INFO - History cleared. Kept 2 system messages.\n",
      "2025-04-24 06:27:15,772 - cellmage.chat_manager - INFO - Sending message to LLM with 3 messages in context\n",
      "2025-04-24 06:27:15,781 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 3 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with persona: creative_writer\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:27:28,606 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.3\n",
      "2025-04-24 06:27:28,607 - cellmage.adapters.direct_client - INFO - [Override] Setting 'top_k' = 40\n",
      "2025-04-24 06:27:28,607 - cellmage.chat_manager - INFO - Default persona set to 'data_analyst'\n",
      "2025-04-24 06:27:28,608 - cellmage.history_manager - INFO - History cleared. Kept 2 system messages.\n",
      "2025-04-24 06:27:28,608 - cellmage.chat_manager - INFO - Sending message to LLM with 3 messages in context\n",
      "2025-04-24 06:27:28,608 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 3 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response summary: Okay, let's break down machine learning in a simple way, like teaching something new.\n",
      "\n",
      "Imagine you want to teach a computer to recognize cats and dogs in pictures.\n",
      "\n",
      "**The \"Old Way\" (Traditional Programming):**\n",
      "\n",
      "You would have to look at cats and dogs yourself and write down a huge list of rules for ...\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing with persona: data_analyst\n",
      "--------------------------------------------------\n",
      "Response summary: Okay, imagine you want to teach a computer to do something, but instead of giving it a step-by-step instruction manual for *every single possible situation*, you want it to *figure things out* on its own by looking at examples.\n",
      "\n",
      "That's the core idea behind **Machine Learning**.\n",
      "\n",
      "Think about how *you...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a new conversation with clear history\n",
    "chat_manager.clear_history()\n",
    "\n",
    "# Define a consistent test prompt\n",
    "test_prompt = \"Explain the concept of machine learning to a beginner\"\n",
    "\n",
    "# Test with different personas\n",
    "personas = [\"creative_writer\", \"data_analyst\"]\n",
    "\n",
    "for persona in personas:\n",
    "    print(f\"\\nTesting with persona: {persona}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Set the persona\n",
    "    chat_manager.set_default_persona(persona)\n",
    "    chat_manager.clear_history(keep_system=True)  # Keep only the system message\n",
    "\n",
    "    # Send the prompt\n",
    "    response = chat_manager.chat(test_prompt, stream=False)\n",
    "\n",
    "    # Print a summary of the response (first 300 characters)\n",
    "    print(f\"Response summary: {response[:300]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4263f62",
   "metadata": {},
   "source": [
    "## Saving and Loading Conversations\n",
    "\n",
    "Let's test saving and loading conversations with Gemini 2.5 Flash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d60d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:27:39,108 - cellmage.history_manager - INFO - History cleared. Kept 2 system messages.\n",
      "2025-04-24 06:27:39,110 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.8\n",
      "2025-04-24 06:27:39,111 - cellmage.adapters.direct_client - INFO - [Override] Setting 'top_p' = 0.95\n",
      "2025-04-24 06:27:39,111 - cellmage.chat_manager - INFO - Default persona set to 'creative_writer'\n",
      "2025-04-24 06:27:39,112 - cellmage.chat_manager - INFO - Sending message to LLM with 3 messages in context\n",
      "2025-04-24 06:27:39,112 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 3 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new conversation about storytelling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 06:27:53,385 - cellmage.chat_manager - INFO - Sending message to LLM with 5 messages in context\n",
      "2025-04-24 06:27:53,386 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 5 messages\n",
      "2025-04-24 06:28:09,273 - cellmage.storage.memory_store - INFO - Saved conversation to memory with ID: gemini25flash_storytelling_convo\n",
      "2025-04-24 06:28:09,274 - cellmage.history_manager - INFO - History cleared. Kept 2 system messages.\n",
      "2025-04-24 06:28:09,275 - cellmage.history_manager - INFO - Loaded conversation from gemini25flash_storytelling_convo with 6 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation saved to: gemini25flash_storytelling_convo\n",
      "History cleared, current message count: 2\n",
      "Conversation loaded, message count: 6\n",
      "\n",
      "Loaded conversation:\n",
      "2. user: What are the key elements of a compelling story?...\n",
      "3. assistant: Ah, the alchemical blend that turns mere words into worlds and simple sequences into soul-stirring j...\n",
      "4. user: How do I develop interesting characters?...\n",
      "5. assistant: Ah, breathing life into characters! This is arguably one of the most rewarding and crucial aspects o...\n"
     ]
    }
   ],
   "source": [
    "# Create a new conversation with the creative writer persona\n",
    "chat_manager.clear_history()\n",
    "chat_manager.set_default_persona(\"creative_writer\")\n",
    "\n",
    "# Have a brief conversation about storytelling\n",
    "print(\"Creating a new conversation about storytelling...\")\n",
    "chat_manager.chat(\"What are the key elements of a compelling story?\", stream=False)\n",
    "chat_manager.chat(\"How do I develop interesting characters?\", stream=False)\n",
    "\n",
    "# Save the conversation\n",
    "save_path = chat_manager.save_conversation(\"gemini25flash_storytelling_convo\")\n",
    "print(f\"\\nConversation saved to: {save_path}\")\n",
    "\n",
    "# Clear the history\n",
    "chat_manager.clear_history()\n",
    "print(f\"History cleared, current message count: {len(chat_manager.get_history())}\")\n",
    "\n",
    "# Load the conversation back\n",
    "if save_path:\n",
    "    chat_manager.load_conversation(save_path)\n",
    "    print(f\"Conversation loaded, message count: {len(chat_manager.get_history())}\")\n",
    "\n",
    "    # Display the loaded conversation\n",
    "    print(\"\\nLoaded conversation:\")\n",
    "    for i, message in enumerate(chat_manager.get_history()):\n",
    "        if message.role != \"system\":  # Skip system message for brevity\n",
    "            print(f\"{i}. {message.role}: {message.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9cc985",
   "metadata": {},
   "source": [
    "## Conclusion: Gemini 2.5 Flash Performance\n",
    "\n",
    "This notebook has demonstrated the capabilities of Gemini 2.5 Flash when used with the cellmage library:\n",
    "\n",
    "1. **Response Speed**: Gemini 2.5 Flash generally provides very quick responses, making it excellent for interactive applications.\n",
    "2. **Creative Content**: It performs well on creative tasks with appropriate temperature settings.\n",
    "3. **Technical Accuracy**: With lower temperature settings, it can provide accurate technical information.\n",
    "4. **Multimodal Potential**: While requiring additional implementation, its multimodal capabilities offer interesting possibilities.\n",
    "5. **Function Calling**: Its function calling capabilities can be leveraged with extensions to the cellmage library.\n",
    "\n",
    "**Use Cases for Gemini 2.5 Flash:**\n",
    "- Interactive chatbots requiring low latency\n",
    "- Creative content generation\n",
    "- Technical support and documentation\n",
    "- Data analysis explanations\n",
    "- Applications with budget constraints\n",
    "\n",
    "Gemini 2.5 Flash offers an excellent balance of performance, speed, and cost, making it a versatile option for many applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
