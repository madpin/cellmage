{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ad4a88",
   "metadata": {},
   "source": [
    "# Testing Cellmage with Gemini 2.5 Flash\n",
    "\n",
    "This notebook tests the capabilities of Cellmage with Google's Gemini 2.5 Flash model.\n",
    "\n",
    "**Date:** April 24, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0e552",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Let's set up our development environment and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe45b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:04:49,529 - cellmage - INFO - Cellmage logging initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /Users/tpinto/madpin/cellmage/notebooks\n",
      "Project root directory: /Users/tpinto/madpin/cellmage\n",
      "Added path: /Users/tpinto/madpin/cellmage\n",
      "Cellmage version: 0.1.0\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Skip dotenv loading for testing\n",
    "os.environ[\"CELLMAGE_SKIP_DOTENV\"] = \"1\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Ensure the cellmage package can be imported\n",
    "# Get the absolute path of the current working directory\n",
    "notebook_dir = os.getcwd()\n",
    "# Get the project root directory (parent of the notebook directory)\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "\n",
    "print(f\"Notebook directory: {notebook_dir}\")\n",
    "print(f\"Project root directory: {project_root}\")\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added path: {project_root}\")\n",
    "\n",
    "try:\n",
    "    # Import cellmage\n",
    "    import cellmage\n",
    "\n",
    "    # Check version - handle case where __version__ might not be available\n",
    "    try:\n",
    "        print(f\"Cellmage version: {cellmage.__version__}\")\n",
    "    except AttributeError:\n",
    "        print(\"Cellmage imported successfully, but version information is not available\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Error importing cellmage: {e}\")\n",
    "    print(\"\\nDebug information:\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Python path: {sys.path}\")\n",
    "    print(\"\\nTry running this notebook from the project root directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c837ef9",
   "metadata": {},
   "source": [
    "## Setting Up the Gemini LLM Client\n",
    "\n",
    "Now we'll configure the LLM client to use Google's Gemini 2.5 Flash model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf7302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:04:49,576 - cellmage.adapters.direct_client - INFO - [Override] Setting 'api_key' = sk-L...xxmA\n",
      "2025-04-24 07:04:49,577 - cellmage.adapters.direct_client - INFO - [Override] Setting 'api_base' = https://litellm.oracle.madpin.dev\n",
      "2025-04-24 07:04:49,577 - cellmage.adapters.direct_client - INFO - [Override] Setting 'model' = gemini-2.5-flash\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM client initialized with model: gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "from cellmage.adapters.direct_client import DirectLLMAdapter\n",
    "\n",
    "# Create an LLM client with Gemini-2.5-flash model\n",
    "# Note: You need to set GOOGLE_API_KEY in your environment\n",
    "llm_client = DirectLLMAdapter(default_model=\"gemini-2.5-flash\")\n",
    "\n",
    "print(\"LLM client initialized with model: gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860cddea",
   "metadata": {},
   "source": [
    "## Creating Specialized Personas for Gemini\n",
    "\n",
    "Let's create some specialized personas that work well with Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8682a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:04:49,677 - cellmage.resources.memory_loader - INFO - Added persona 'code_expert' to memory\n",
      "2025-04-24 07:04:49,678 - cellmage.resources.memory_loader - INFO - Added persona 'creative_writer' to memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available personas: ['code_expert', 'creative_writer']\n"
     ]
    }
   ],
   "source": [
    "from cellmage.resources.memory_loader import MemoryLoader\n",
    "from cellmage.storage.memory_store import MemoryStore\n",
    "\n",
    "# Create in-memory components for testing\n",
    "persona_loader = MemoryLoader()\n",
    "snippet_provider = MemoryLoader()\n",
    "history_store = MemoryStore()\n",
    "\n",
    "# Create specialized personas\n",
    "persona_loader.add_persona(\n",
    "    name=\"code_expert\",\n",
    "    system_message=\"You are a programming expert who specializes in writing clean, efficient code. Provide detailed explanations and always suggest best practices.\",\n",
    "    config={\"temperature\": 0.3, \"top_p\": 0.95},\n",
    ")\n",
    "\n",
    "persona_loader.add_persona(\n",
    "    name=\"creative_writer\",\n",
    "    system_message=\"You are a creative writing assistant who helps draft engaging stories and content. Be imaginative and suggest unique perspectives.\",\n",
    "    config={\"temperature\": 0.9, \"top_p\": 0.98},\n",
    ")\n",
    "\n",
    "# List available personas\n",
    "print(f\"Available personas: {persona_loader.list_personas()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e99c50",
   "metadata": {},
   "source": [
    "## Creating a Chat Manager with Gemini\n",
    "\n",
    "Now let's set up a Chat Manager with the Gemini model and our personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e490f4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:04:49,688 - cellmage.chat_manager - INFO - Initializing ChatManager\n",
      "2025-04-24 07:04:49,696 - cellmage.chat_manager - INFO - ChatManager initialized\n",
      "2025-04-24 07:04:49,700 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.3\n",
      "2025-04-24 07:04:49,701 - cellmage.adapters.direct_client - INFO - [Override] Setting 'top_p' = 0.95\n",
      "2025-04-24 07:04:49,702 - cellmage.chat_manager - INFO - Default persona set to 'code_expert'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat manager initialized with 'code_expert' persona\n"
     ]
    }
   ],
   "source": [
    "# Create a chat manager\n",
    "chat_manager = cellmage.ChatManager(\n",
    "    llm_client=llm_client,\n",
    "    persona_loader=persona_loader,\n",
    "    snippet_provider=snippet_provider,\n",
    "    history_store=history_store,\n",
    ")\n",
    "\n",
    "# Set default persona to code_expert\n",
    "chat_manager.set_default_persona(\"code_expert\")\n",
    "\n",
    "print(\"Chat manager initialized with 'code_expert' persona\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509e5790",
   "metadata": {},
   "source": [
    "## Testing Code Generation with Gemini\n",
    "\n",
    "Let's test Gemini's code generation capabilities through our chat manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8f2823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:04:49,708 - cellmage.chat_manager - INFO - Sending message to LLM with 2 messages in context\n",
      "2025-04-24 07:04:49,709 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 2 messages\n"
     ]
    }
   ],
   "source": [
    "# Add a challenging code generation task\n",
    "response = chat_manager.chat(\n",
    "    \"\"\"Write a Python function that acts as a simple text-based calculator. \n",
    "    It should take a string expression like '2 + 3 * 4' and return the calculated result.\n",
    "    Handle addition, subtraction, multiplication, division, and parentheses.\n",
    "    Make sure to respect the order of operations.\"\"\",\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00650056",
   "metadata": {},
   "source": [
    "## Testing Creative Content Generation\n",
    "\n",
    "Now let's switch to our creative writer persona and test content generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a02a8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:05:26,885 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.9\n",
      "2025-04-24 07:05:26,887 - cellmage.adapters.direct_client - INFO - [Override] Setting 'top_p' = 0.98\n",
      "2025-04-24 07:05:26,887 - cellmage.chat_manager - INFO - Default persona set to 'creative_writer'\n",
      "2025-04-24 07:05:26,888 - cellmage.chat_manager - INFO - Sending message to LLM with 4 messages in context\n",
      "2025-04-24 07:05:26,888 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 4 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to 'creative_writer' persona\n"
     ]
    }
   ],
   "source": [
    "# Switch to creative_writer persona\n",
    "chat_manager.set_default_persona(\"creative_writer\")\n",
    "print(\"Switched to 'creative_writer' persona\")\n",
    "\n",
    "# Generate creative content\n",
    "response = chat_manager.chat(\n",
    "    \"\"\"Write a short story about a programmer who discovers an AI that can \n",
    "    predict the future, but only for trivial events. Make it humorous and \n",
    "    include a surprising twist at the end.\"\"\",\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5045d22",
   "metadata": {},
   "source": [
    "## Adding Context with Code Snippets\n",
    "\n",
    "Let's test how Gemini performs with code snippets as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bad0c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:06:11,277 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.3\n",
      "2025-04-24 07:06:11,279 - cellmage.adapters.direct_client - INFO - [Override] Setting 'top_p' = 0.95\n",
      "2025-04-24 07:06:11,280 - cellmage.chat_manager - INFO - Default persona set to 'code_expert'\n",
      "2025-04-24 07:06:11,281 - cellmage.resources.memory_loader - INFO - Added snippet 'buggy_sort' to memory\n",
      "2025-04-24 07:06:11,281 - cellmage.chat_manager - INFO - Added snippet 'buggy_sort' as system message\n",
      "2025-04-24 07:06:11,282 - cellmage.chat_manager - INFO - Sending message to LLM with 7 messages in context\n",
      "2025-04-24 07:06:11,282 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 7 messages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch back to code_expert persona\n",
    "chat_manager.set_default_persona(\"code_expert\")\n",
    "\n",
    "# Create a code snippet with a bug\n",
    "snippet_provider.add_snippet(\n",
    "    name=\"buggy_sort\",\n",
    "    content=\"\"\"```python\n",
    "def bubble_sort(arr):\n",
    "    n = len(arr)\n",
    "    for i in range(n):\n",
    "        for j in range(0, n-i-1):\n",
    "            if arr[j] > arr[j+1]:\n",
    "                arr[j], arr[j+1] = arr[j], arr[j+1]  # Bug: This line is correct but there's a logical error\n",
    "    return arr\n",
    "\n",
    "# Test the function\n",
    "test_array = [64, 34, 25, 12, 22, 11, 90]\n",
    "sorted_array = bubble_sort(test_array)\n",
    "print(\"Sorted array:\", sorted_array)\n",
    "```\"\"\",\n",
    ")\n",
    "\n",
    "# Add the snippet to the conversation\n",
    "chat_manager.add_snippet(\"buggy_sort\")\n",
    "\n",
    "# Ask about the bug\n",
    "response = chat_manager.chat(\n",
    "    \"\"\"There's a logical error in this bubble sort implementation. \n",
    "    The function seems to work correctly for this test case, but it's not implemented optimally. \n",
    "    Can you identify the issue and suggest a fix?\"\"\",\n",
    "    stream=True,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570075a",
   "metadata": {},
   "source": [
    "## Comparing Gemini's Performance\n",
    "\n",
    "Let's create a complex query and test it with temperature variations to see how Gemini's responses differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "032275b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:06:27,726 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.1\n",
      "2025-04-24 07:06:27,727 - cellmage.chat_manager - INFO - Sending message to LLM with 9 messages in context\n",
      "2025-04-24 07:06:27,728 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 9 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Response with temperature = 0.1 =====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You are absolutely right! There is indeed a significant logical error in the provided bubble sort implementation, even though the test case might misleadingly appear to work (it actually just returns the original unsorted array because no swaps happen).\\n\\nLet\\'s break down the issue and how to fix it.\\n\\n**The Logical Error**\\n\\nThe core of the bubble sort algorithm is the **swapping** of adjacent elements if they are in the wrong order. The provided code checks the condition `if arr[j] > arr[j+1]:`, which is correct for determining if a swap is needed. However, the line intended to perform the swap is:\\n\\n```python\\narr[j], arr[j+1] = arr[j], arr[j+1]\\n```\\n\\nThis line does **not** perform a swap. It simply assigns the value of `arr[j]` back to `arr[j]` and the value of `arr[j+1]` back to `arr[j+1]`. In essence, it does nothing to change the order of the elements in the array.\\n\\nBecause no swaps ever occur, the array remains in its original unsorted state after the function completes. The test case outputting `[64, 34, 25, 12, 22, 11, 90]` is actually the *original* array, not a sorted one.\\n\\n**The Fix**\\n\\nTo fix this, you need to replace the incorrect assignment with a proper swap operation. In Python, the most common and Pythonic way to swap two variables `a` and `b` is using tuple packing and unpacking: `a, b = b, a`.\\n\\nApplying this to the array elements, the line inside the `if` block should be:\\n\\n```python\\narr[j], arr[j+1] = arr[j+1], arr[j]\\n```\\n\\nThis line correctly assigns the value of `arr[j+1]` to `arr[j]` and the value of `arr[j]` to `arr[j+1]`, effectively swapping their positions in the list.\\n\\n**Corrected Bubble Sort Implementation**\\n\\nHere is the corrected version of the `bubble_sort` function:\\n\\n```python\\ndef bubble_sort(arr):\\n    \"\"\"\\n    Sorts a list of numbers using the Bubble Sort algorithm.\\n\\n    Args:\\n        arr: A list of comparable elements (e.g., numbers).\\n\\n    Returns:\\n        The sorted list (the sort is performed in-place).\\n    \"\"\"\\n    n = len(arr)\\n    # Outer loop controls the number of passes\\n    # After each pass \\'i\\', the last \\'i\\' elements are in place\\n    for i in range(n):\\n        # Inner loop for comparisons and swaps\\n        # We iterate up to n-i-1 because the last \\'i\\' elements are already sorted\\n        for j in range(0, n-i-1):\\n            # Compare adjacent elements\\n            if arr[j] > arr[j+1]:\\n                # Swap arr[j] and arr[j+1] if they are in the wrong order\\n                arr[j], arr[j+1] = arr[j+1], arr[j] # Correct Swap!\\n    return arr\\n\\n# Test the corrected function\\ntest_array = [64, 34, 25, 12, 22, 11, 90]\\nsorted_array = bubble_sort(test_array)\\nprint(\"Sorted array:\", sorted_array) # Expected output: [11, 12, 22, 25, 34, 64, 90]\\n```\\n\\n**Best Practice: Optimization (Early Termination)**\\n\\nWhile the above fixes the logical error, bubble sort has a simple optimization. If a pass through the inner loop completes *without* performing any swaps, it means the array is already sorted, and we can stop early. This avoids unnecessary passes.\\n\\nHere\\'s the optimized version:\\n\\n```python\\ndef bubble_sort_optimized(arr):\\n    \"\"\"\\n    Sorts a list of numbers using the Bubble Sort algorithm with early termination.\\n\\n    Args:\\n        arr: A list of comparable elements (e.g., numbers).\\n\\n    Returns:\\n        The sorted list (the sort is performed in-place).\\n    \"\"\"\\n    n = len(arr)\\n    # Outer loop\\n    for i in range(n):\\n        # Flag to check if any swaps occurred in this pass\\n        swapped = False\\n        # Inner loop\\n        for j in range(0, n-i-1):\\n            if arr[j] > arr[j+1]:\\n                # Swap\\n                arr[j], arr[j+1] = arr[j+1], arr[j]\\n                swapped = True # Mark that a swap occurred\\n        \\n        # If no swaps occurred in this pass, the array is sorted\\n        if not swapped:\\n            break # Terminate early\\n\\n    return arr\\n\\n# Test the optimized function\\ntest_array_optimized = [64, 34, 25, 12, 22, 11, 90]\\nsorted_array_optimized = bubble_sort_optimized(test_array_optimized)\\nprint(\"Sorted array (optimized):\", sorted_array_optimized) # Expected output: [11, 12, 22, 25, 34, 64, 90]\\n\\n# Test with an already sorted array to see early termination\\ntest_array_pre_sorted = [1, 2, 3, 4, 5]\\nprint(\"Sorted pre-sorted array:\", bubble_sort_optimized(test_array_pre_sorted)) # Should sort quickly\\n```\\n\\n**Summary of the Issue and Fix:**\\n\\n*   **Issue:** The line `arr[j], arr[j+1] = arr[j], arr[j+1]` incorrectly attempts to swap elements by assigning them back to themselves, resulting in no actual sorting.\\n*   **Fix:** Replace the incorrect line with the proper Python swap syntax: `arr[j], arr[j+1] = arr[j+1], arr[j]`.\\n*   **Best Practice:** Add an early termination check (`swapped` flag) to stop sorting passes once the array is fully sorted.\\n\\nRemember that while bubble sort is simple to understand, it is generally inefficient for large datasets (O(n^2) time complexity). For practical purposes in Python, using the built-in `sorted()` function or the list\\'s `.sort()` method (which use Timsort, a much faster hybrid algorithm) is highly recommended.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:06:42,763 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.9\n",
      "2025-04-24 07:06:42,765 - cellmage.history_manager - INFO - History cleared. Kept 2 system messages.\n",
      "2025-04-24 07:06:42,766 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.3\n",
      "2025-04-24 07:06:42,766 - cellmage.adapters.direct_client - INFO - [Override] Setting 'top_p' = 0.95\n",
      "2025-04-24 07:06:42,766 - cellmage.chat_manager - INFO - Default persona set to 'code_expert'\n",
      "2025-04-24 07:06:42,767 - cellmage.chat_manager - INFO - Sending message to LLM with 3 messages in context\n",
      "2025-04-24 07:06:42,767 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 3 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Response with temperature = 0.9 =====\n"
     ]
    }
   ],
   "source": [
    "# Set a lower temperature for precise answers\n",
    "import cellmage.integrations\n",
    "\n",
    "\n",
    "chat_manager.set_override(\"temperature\", 0.1)\n",
    "\n",
    "# Complex technical question\n",
    "tech_query = \"\"\"Explain the differences between synchronous and asynchronous programming \n",
    "                 paradigms in Python, including when to use each and their advantages \n",
    "                 and disadvantages. Include code examples.\"\"\"\n",
    "\n",
    "# Get response with low temperature\n",
    "print(\"===== Response with temperature = 0.1 =====\")\n",
    "precise_response = chat_manager.chat(tech_query, stream=False)\n",
    "\n",
    "cellmage.integrations.ipython_magic.display(precise_response)\n",
    "# Set a higher temperature for more creative answers\n",
    "chat_manager.set_override(\"temperature\", 0.9)\n",
    "\n",
    "# Clear history to get a fresh response\n",
    "chat_manager.clear_history()\n",
    "chat_manager.set_default_persona(\"code_expert\")\n",
    "\n",
    "# Get response with high temperature\n",
    "print(\"\\n===== Response with temperature = 0.9 =====\")\n",
    "creative_response = chat_manager.chat(tech_query, stream=True)\n",
    "# creative_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc58b94",
   "metadata": {},
   "source": [
    "## Testing Gemini's Contextual Understanding\n",
    "\n",
    "Let's test Gemini's ability to maintain context over a multi-turn conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6753ecf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:07:06,608 - cellmage.history_manager - INFO - History cleared. Kept 2 system messages.\n",
      "2025-04-24 07:07:06,609 - cellmage.adapters.direct_client - INFO - [Override] Setting 'temperature' = 0.3\n",
      "2025-04-24 07:07:06,610 - cellmage.adapters.direct_client - INFO - [Override] Setting 'top_p' = 0.95\n",
      "2025-04-24 07:07:06,610 - cellmage.chat_manager - INFO - Default persona set to 'code_expert'\n",
      "2025-04-24 07:07:06,611 - cellmage.chat_manager - INFO - Sending message to LLM with 3 messages in context\n",
      "2025-04-24 07:07:06,612 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 3 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== First Turn =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:07:19,293 - cellmage.chat_manager - INFO - Sending message to LLM with 5 messages in context\n",
      "2025-04-24 07:07:19,294 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 5 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Second Turn =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:07:28,900 - cellmage.chat_manager - INFO - Sending message to LLM with 7 messages in context\n",
      "2025-04-24 07:07:28,902 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 7 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Third Turn =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:07:55,327 - cellmage.chat_manager - INFO - Sending message to LLM with 9 messages in context\n",
      "2025-04-24 07:07:55,328 - cellmage.adapters.direct_client - INFO - Calling model 'gemini-2.5-flash' with 9 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fourth Turn =====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Okay, let\\'s implement the **Memoization** technique using Python\\'s built-in `functools.lru_cache` decorator. This is the most common and idiomatic way to add memoization to a recursive function in Python.\\n\\nWe\\'ll take the original recursive Fibonacci function and add the decorator.\\n\\n```python\\nimport functools\\n# import sys # Optional: Uncomment to potentially increase recursion depth for testing, use with caution!\\n# sys.setrecursionlimit(2000)\\n\\n@functools.lru_cache(max_size=None) # This is the memoization decorator\\ndef fibonacci_memoized(n):\\n    \"\"\"\\n    Calculates the n-th Fibonacci number using recursion with memoization.\\n    This version is significantly faster than the naive recursive version\\n    for larger n because it avoids redundant calculations.\\n    \"\"\"\\n    # 1. Base Cases:\\n    # These are the stopping conditions.\\n    if n <= 1:\\n        return n\\n\\n    # 2. Recursive Step:\\n    # The function calls itself with smaller values.\\n    # lru_cache automatically stores and retrieves results for n-1 and n-2.\\n    else:\\n        return fibonacci_memoized(n - 1) + fibonacci_memoized(n - 2)\\n\\n# --- Example Usage ---\\nprint(f\"Memoized Fibonacci(0): {fibonacci_memoized(0)}\")\\nprint(f\"Memoized Fibonacci(1): {fibonacci_memoized(1)}\")\\nprint(f\"Memoized Fibonacci(2): {fibonacci_memoized(2)}\")\\nprint(f\"Memoized Fibonacci(3): {fibonacci_memoized(3)}\")\\nprint(f\"Memoized Fibonacci(4): {fibonacci_memoized(4)}\")\\nprint(f\"Memoized Fibonacci(5): {fibonacci_memoized(5)}\")\\nprint(f\"Memoized Fibonacci(10): {fibonacci_memoized(10)}\")\\n\\n# Demonstrate the performance improvement for larger numbers\\n# The naive recursive version would be extremely slow or crash here\\nprint(f\"Memoized Fibonacci(30): {fibonacci_memoized(30)}\")\\nprint(f\"Memoized Fibonacci(50): {fibonacci_memoized(50)}\")\\n# print(f\"Memoized Fibonacci(1000): {fibonacci_memoized(1000)}\") # Might still hit recursion limit depending on system\\n```\\n\\n**Explanation of the Optimization:**\\n\\n1.  **`import functools`**: We import the necessary module that contains the `lru_cache` decorator.\\n2.  **`@functools.lru_cache(max_size=None)`**: This is the core of the optimization.\\n    *   `@`: This symbol indicates that `lru_cache` is a decorator. Decorators are a way to modify or enhance a function or method.\\n    *   `functools.lru_cache`: This is the Least Recently Used (LRU) cache decorator. It wraps the function (`fibonacci_memoized` in this case).\\n    *   `max_size=None`: This argument tells the cache to store results for *all* unique inputs it encounters, without a limit on the number of entries. For Fibonacci, this is fine as we only call it with `n, n-1, n-2, ... 0`.\\n3.  **How it works:**\\n    *   When `fibonacci_memoized(n)` is called for the first time with a specific value of `n`, the decorator executes the function body.\\n    *   Before returning the result, the decorator stores the pair `(n, result)` in an internal dictionary (the cache).\\n    *   If `fibonacci_memoized(n)` is called again with the *same* value of `n`, the decorator intercepts the call, finds the result in its cache, and immediately returns the cached result *without* executing the function body again.\\n4.  **Benefit for Fibonacci:** In the naive recursive Fibonacci, calculating `fib(5)` requires calculating `fib(4)` and `fib(3)`. Calculating `fib(4)` requires `fib(3)` and `fib(2)`. Notice `fib(3)` is calculated twice. For larger `n`, many values are recalculated exponentially many times. Memoization ensures each `fib(k)` is calculated only *once*.\\n5.  **Avoiding Stack Overflow (Indirectly):** While the recursive calls still happen, the number of *actual computations* and the *time spent* in the function body are drastically reduced. For `fibonacci_memoized(50)`, the function body for each `k` from 50 down to 0 is executed only once. The stack depth still goes down to the base case (around 50 levels deep for `fib(50)`), but the total number of function calls is linear (`O(n)`) instead of exponential (`O(2^n)`). This makes it feasible to calculate Fibonacci for much larger `n` before hitting Python\\'s recursion depth limit compared to the naive version. It doesn\\'t *eliminate* the recursion depth limit itself, but it pushes it to much higher values of `n`.\\n\\nThis memoized version is a great example of how a simple addition can turn an inefficient recursive function into a highly performant one, while still retaining the clarity of the recursive definition.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear history for a fresh conversation\n",
    "chat_manager.clear_history()\n",
    "chat_manager.set_default_persona(\"code_expert\")\n",
    "\n",
    "# Let's have a multi-turn conversation about a specific topic\n",
    "print(\"===== First Turn =====\")\n",
    "chat_manager.chat(\"What is a recursive function in programming?\", stream=False)\n",
    "\n",
    "print(\"\\n===== Second Turn =====\")\n",
    "chat_manager.chat(\"Can you give me an example using Python?\", stream=False)\n",
    "\n",
    "print(\"\\n===== Third Turn =====\")\n",
    "chat_manager.chat(\"What are some ways to optimize it to avoid stack overflow?\", stream=False)\n",
    "\n",
    "print(\"\\n===== Fourth Turn =====\")\n",
    "chat_manager.chat(\"Implement one of those optimization techniques in the example you provided.\", stream=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd43cf1",
   "metadata": {},
   "source": [
    "## Saving Multi-Turn Conversations\n",
    "\n",
    "Let's save our multi-turn conversation and see how it's stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "314f8793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 07:08:05,408 - cellmage.storage.memory_store - INFO - Saved conversation to memory with ID: gemini_recursion_conversation\n",
      "2025-04-24 07:08:05,416 - cellmage.history_manager - INFO - History cleared. Kept 2 system messages.\n",
      "2025-04-24 07:08:05,421 - cellmage.history_manager - INFO - Loaded conversation from gemini_recursion_conversation with 10 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation saved to: gemini_recursion_conversation\n",
      "History cleared, current message count: 2\n",
      "Conversation loaded, message count: 10\n",
      "\n",
      "===== Loaded Conversation Summary =====\n",
      "[user] What is a recursive function in programming?\n",
      "[assistant] Okay, let's break down what a recursive function i...\n",
      "[user] Can you give me an example using Python?\n",
      "[assistant] Okay, let's use the **Fibonacci sequence** as a cl...\n",
      "[user] What are some ways to optimize it to avoid stack o...\n",
      "[assistant] Okay, let's look at ways to optimize the recursive...\n",
      "[user] Implement one of those optimization techniques in ...\n",
      "[assistant] Okay, let's implement the **Memoization** techniqu...\n"
     ]
    }
   ],
   "source": [
    "# Save the conversation\n",
    "save_path = chat_manager.save_conversation(\"gemini_recursion_conversation\")\n",
    "print(f\"Conversation saved to: {save_path}\")\n",
    "\n",
    "# Clear the history\n",
    "chat_manager.clear_history()\n",
    "print(f\"History cleared, current message count: {len(chat_manager.get_history())}\")\n",
    "\n",
    "# Load the conversation back\n",
    "if save_path:\n",
    "    chat_manager.load_conversation(save_path)\n",
    "    loaded_history = chat_manager.get_history()\n",
    "    print(f\"Conversation loaded, message count: {len(loaded_history)}\")\n",
    "\n",
    "    # Display the loaded conversation summary\n",
    "    print(\"\\n===== Loaded Conversation Summary =====\")\n",
    "    for i, msg in enumerate(loaded_history):\n",
    "        if msg.role != \"system\":\n",
    "            content_preview = msg.content[:50] + \"...\" if len(msg.content) > 50 else msg.content\n",
    "            print(f\"[{msg.role}] {content_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49efc698",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated the capabilities of Cellmage with the Gemini 2.5 Flash model, including:\n",
    "\n",
    "1. Setting up a Gemini model configuration\n",
    "2. Creating specialized personas\n",
    "3. Testing code generation capabilities\n",
    "4. Testing creative content generation\n",
    "5. Using code snippets to provide context\n",
    "6. Testing contextual understanding across multiple conversation turns\n",
    "7. Saving and loading multi-turn conversations\n",
    "\n",
    "The Gemini 2.5 Flash model shows strong capabilities across both technical and creative tasks within the Cellmage framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
