{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a84f90b3",
   "metadata": {},
   "source": [
    "# üßô CellMage Persistent Chat Example ‚ú®\n",
    "\n",
    "This notebook demonstrates how to use CellMage's persistent chat capabilities, which allow you to maintain conversation context and persona settings across cells and sessions.\n",
    "\n",
    "**Date:** April 26, 2025\n",
    "\n",
    "## What is Persistent Chat?\n",
    "\n",
    "Persistent chat in CellMage combines two powerful features:\n",
    "1. **Context Retention**: The LLM remembers previous interactions in your conversation\n",
    "2. **Ambient Mode**: Process regular cells as prompts to the LLM without magic commands\n",
    "\n",
    "This creates a seamless chat experience where you can simply type natural language in code cells and get responses while maintaining the full conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bf044e",
   "metadata": {},
   "source": [
    "## 1. Setting Up CellMage\n",
    "\n",
    "First, let's load the CellMage extension and ensure it's properly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c35fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NotebookLLM Magics loaded. Use %llm_config and %%llm.\n",
      "   For ambient mode, try %llm_config_persistent to process all cells as LLM prompts.\n",
      "CellMage version: 0.1.0\n"
     ]
    }
   ],
   "source": [
    "# Load the extension\n",
    "%load_ext cellmage\n",
    "\n",
    "# Import the necessary modules\n",
    "import cellmage\n",
    "print(f\"CellMage version: {cellmage.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848ea43b",
   "metadata": {},
   "source": [
    "## 2. Setting Up Personas and Context\n",
    "\n",
    "One of the most powerful features of CellMage is the ability to define personas - specialized configurations that include system prompts and LLM parameters. These help shape the LLM's responses to better suit your needs.\n",
    "\n",
    "In this example, we'll set up a persistent chat with:\n",
    "- A specific model: `gpt-4.1-mini`\n",
    "- The \"coder\" persona: specialized for coding tasks\n",
    "- System snippets: additional context about \"Thiago\" and \"EVO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14aa928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 14:48:14,681 - cellmage.resources.file_loader - WARNING - Snippet 'Thiago.md' not found at llm_snippets/Thiago.md\n",
      "2025-04-27 14:48:14,682 - cellmage.chat_manager - WARNING - Snippet 'Thiago' not found\n",
      "2025-04-27 14:48:14,682 - cellmage.resources.file_loader - WARNING - Snippet 'EVO.md' not found at llm_snippets/EVO.md\n",
      "2025-04-27 14:48:14,682 - cellmage.chat_manager - WARNING - Snippet 'EVO' not found\n",
      "2025-04-27 14:48:14,683 - cellmage.resources.file_loader - WARNING - Persona 'coder' not found in llm_personas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Default model set to: gpt-4.1-mini\n",
      "‚ö†Ô∏è Warning: Could not add system snippet 'Thiago'.\n",
      "‚ö†Ô∏è Warning: Could not add system snippet 'EVO'.\n",
      "‚ùå Error: Persona 'coder' not found.\n",
      "‚úÖ Ambient mode ENABLED. All cells will now be processed as LLM prompts unless they start with % or !.\n",
      "   Run %disable_llm_config_persistent to disable ambient mode.\n"
     ]
    }
   ],
   "source": [
    "# Configure the CellMage environment for persistent chat\n",
    "%llm_config_persistent --model gpt-4.1-mini --persona coder --sys-snippet Thiago --sys-snippet EVO\n",
    "\n",
    "# Note: %llm_config_persistent combines:\n",
    "# 1. %llm_config (normal configuration)\n",
    "# 2. %llm_setup_forever (enabling ambient mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a748e7",
   "metadata": {},
   "source": [
    "### What Are System Snippets?\n",
    "\n",
    "System snippets provide additional context to the LLM as part of the system message. They can include:\n",
    "- Information about users or entities (like \"Thiago\" in our example)\n",
    "- Domain knowledge (like \"EVO\" - a project or concept)\n",
    "- Instructions for how the LLM should respond\n",
    "- Code examples or other technical context\n",
    "\n",
    "In this example, we've included two snippet files that should be located in your `snippets/` directory:\n",
    "- `Thiago.md`: Contains information about a person named Thiago\n",
    "- `EVO.md`: Contains information about a concept or project called EVO\n",
    "\n",
    "Let's examine the active persona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5590393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Persona: None\n",
      "  To set a persona, use: %llm_config --persona <name>\n",
      "  To list available personas, use: %llm_config --list-personas\n"
     ]
    }
   ],
   "source": [
    "# Display the active persona configuration\n",
    "%llm_config --show-persona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ead7c0",
   "metadata": {},
   "source": [
    "## 3. Starting a Persistent Chat\n",
    "\n",
    "Now that ambient mode is enabled with our persona and snippets, we can start chatting. Simply type natural language in code cells and run them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f6555f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"background-color: #f1f8e9; border: 1px solid #c5e1a5; color: #33691e;\n",
       "                    padding: 3px 6px; margin-top: 4px; border-radius: 3px; font-family: monospace; \n",
       "                    font-size: 0.75em; line-height: 1.2; display: inline-block; opacity: 0.85;\">\n",
       "            ‚úì gpt-4.1-mini ‚Ä¢ 1.13s ‚Ä¢ 3‚Üì/8‚Üë tokens\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Hello there!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d651f0",
   "metadata": {},
   "source": [
    "### Viewing Conversation History\n",
    "\n",
    "CellMage maintains a history of your conversation. You can view this history at any time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f040f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- History (2 messages) ---\n",
      "[0] USER: Hello there!\n",
      "    (ID: ...8abe9a, Cell: ZQ%3D%3D, Exec: 4)\n",
      "[1] ASSISTANT: Hello! How can I assist you today?\n",
      "    (ID: ...f84d45, Cell: ZQ%3D%3D, Exec: 4)\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display the current conversation history\n",
    "%llm_config --show-history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af0471",
   "metadata": {},
   "source": [
    "## 4. Contextual Awareness\n",
    "\n",
    "One of the key features of persistent chat is that the LLM maintains context from previous interactions. Let's test this by providing some information and then asking the LLM to recall it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d106032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Thiago! How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"background-color: #f1f8e9; border: 1px solid #c5e1a5; color: #33691e;\n",
       "                    padding: 3px 6px; margin-top: 4px; border-radius: 3px; font-family: monospace; \n",
       "                    font-size: 0.75em; line-height: 1.2; display: inline-block; opacity: 0.85;\">\n",
       "            ‚úì gpt-4.1-mini ‚Ä¢ 0.78s ‚Ä¢ 16‚Üì/12‚Üë tokens\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "My name is Thiago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc10dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Thiago. How can I assist you further?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"background-color: #f1f8e9; border: 1px solid #c5e1a5; color: #33691e;\n",
       "                    padding: 3px 6px; margin-top: 4px; border-radius: 3px; font-family: monospace; \n",
       "                    font-size: 0.75em; line-height: 1.2; display: inline-block; opacity: 0.85;\">\n",
       "            ‚úì gpt-4.1-mini ‚Ä¢ 0.83s ‚Ä¢ 33‚Üì/12‚Üë tokens\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "What's my name?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc7adb",
   "metadata": {},
   "source": [
    "Notice how the LLM remembers the name from the previous interaction. This contextual awareness comes from two sources:\n",
    "\n",
    "1. **System Context**: Information provided in the system snippets (Thiago.md)\n",
    "2. **Conversation History**: Previous turns in the conversation\n",
    "\n",
    "Let's check the full history again to see how it's maintaining context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6195d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- History (6 messages) ---\n",
      "[0] USER: Hello there!\n",
      "    (ID: ...8abe9a, Cell: ZQ%3D%3D, Exec: 4)\n",
      "[1] ASSISTANT: Hello! How can I assist you today?\n",
      "    (ID: ...f84d45, Cell: ZQ%3D%3D, Exec: 4)\n",
      "[2] USER: My name is Thiago\n",
      "    (ID: ...1f10b1, Cell: ZQ%3D%3D, Exec: 6)\n",
      "[3] ASSISTANT: Nice to meet you, Thiago! How can I help you today?\n",
      "    (ID: ...c16e80, Cell: ZQ%3D%3D, Exec: 6)\n",
      "[4] USER: What's my name?\n",
      "    (ID: ...d4fc7b, Cell: ZQ%3D%3D, Exec: 7)\n",
      "[5] ASSISTANT: Your name is Thiago. How can I assist you further?\n",
      "    (ID: ...744136, Cell: ZQ%3D%3D, Exec: 7)\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "%llm_config --show-history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dcd466",
   "metadata": {},
   "source": [
    "## 5. Creative Tasks in Persistent Chats\n",
    "\n",
    "Persistent chats are particularly useful for creative tasks like storytelling, where the LLM can build upon previous context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569ac772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Here‚Äôs a short story for you:\n",
       "\n",
       "Thiago was a talented software engineer passionate about artificial intelligence. Every day, he dove into his AI project, designing algorithms that could learn and adapt like a human mind. One evening, after weeks of debugging and testing, Thiago finally watched as his creation recognized patterns and made decisions on its own. Proud but humble, he knew this was just the beginning of a journey that could change the world. With a spark in his eye, Thiago smiled‚Äîready to take on the next challenge.\n",
       "\n",
       "Would you like me to make the story longer or add any specific details?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"background-color: #f1f8e9; border: 1px solid #c5e1a5; color: #33691e;\n",
       "                    padding: 3px 6px; margin-top: 4px; border-radius: 3px; font-family: monospace; \n",
       "                    font-size: 0.75em; line-height: 1.2; display: inline-block; opacity: 0.85;\">\n",
       "            ‚úì gpt-4.1-mini ‚Ä¢ 2.30s ‚Ä¢ 68‚Üì/152‚Üë tokens\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Tell me a short story about a software engineer named Thiago who works on an AI project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84167179",
   "metadata": {},
   "source": [
    "## 6. Advanced: Saving and Loading Conversations\n",
    "\n",
    "CellMage allows you to save your conversations for later use. This is particularly useful for long-running projects where you want to pick up where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22a12550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Unexpected error saving session: 'ChatManager' object has no attribute 'save_session'\n",
      "Conversation saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the current conversation\n",
    "%llm_config --save thiago_conversation\n",
    "\n",
    "print(\"Conversation saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69d6f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chat history cleared.\n",
      "--- History (0 messages) ---\n",
      "(empty)\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "# Clear the current conversation history\n",
    "%llm_config --clear-history\n",
    "\n",
    "# Verify that history is cleared\n",
    "%llm_config --show-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f17e1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Unexpected error loading session 'thiago_conversation': 'ChatManager' object has no attribute 'load_session'\n",
      "--- History (0 messages) ---\n",
      "(empty)\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the saved conversation\n",
    "%llm_config --load thiago_conversation\n",
    "\n",
    "# Verify that the conversation is loaded\n",
    "%llm_config --show-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4aac4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Please provide the portion of the story about Thiago that you'd like me to continue, or let me know some details about him and the context so I can create the continuation accordingly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"background-color: #f1f8e9; border: 1px solid #c5e1a5; color: #33691e;\n",
       "                    padding: 3px 6px; margin-top: 4px; border-radius: 3px; font-family: monospace; \n",
       "                    font-size: 0.75em; line-height: 1.2; display: inline-block; opacity: 0.85;\">\n",
       "            ‚úì gpt-4.1-mini ‚Ä¢ 1.08s ‚Ä¢ 23‚Üì/48‚Üë tokens\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Can you continue the story about Thiago, but have him solve a challenging technical problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353cdd8",
   "metadata": {},
   "source": [
    "## 8. Best Practices for Persistent Chats\n",
    "\n",
    "To get the most out of persistent chats in CellMage, follow these best practices:\n",
    "\n",
    "1. **Use descriptive personas**: Create specialized personas for different tasks (coding, writing, brainstorming)\n",
    "2. **Provide rich context**: Use system snippets to give the LLM the information it needs\n",
    "3. **Save important conversations**: Use the `--save` option to preserve valuable interactions\n",
    "4. **Monitor conversation length**: Very long conversations can hit context limits and increase cost\n",
    "5. **Reset when switching topics**: Use `--clear-history` when starting a new topic\n",
    "6. **Use `--show-history`**: Regularly check what context the LLM is working with\n",
    "\n",
    "## 9. Disabling Persistent Chat\n",
    "\n",
    "When you're done with your persistent chat session, you should disable ambient mode to return to normal notebook operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2401ab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Ambient mode DISABLED. Regular cells will now be executed normally.\n",
      "Persistent chat mode disabled.\n"
     ]
    }
   ],
   "source": [
    "# Disable ambient mode\n",
    "%disable_llm_config_persistent\n",
    "\n",
    "print(\"Persistent chat mode disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e0f72",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Persistent chat in CellMage provides a powerful way to maintain conversational context while interacting with LLMs in your notebooks. Combined with personas and system snippets, it creates a rich, interactive experience tailored to your specific needs.\n",
    "\n",
    "This approach is particularly valuable for:\n",
    "- Extended brainstorming sessions\n",
    "- Creating and refining code with an AI assistant\n",
    "- Educational scenarios where context builds over time\n",
    "- Creative writing and storytelling\n",
    "\n",
    "For more examples and tutorials, check out the [tutorials directory](../tutorials/) or return to the [main README](../README.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11756519",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4151562489.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Tell me a short story about a software engineer named Thiago who works on an AI project.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Tell me a short story about a software engineer named Thiago who works on an AI project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
